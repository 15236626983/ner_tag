[@20 世纪 80 年代末至 90 年代初#Date*]，随着[@计算机技术#ai_tec*]的快速发展，大规模[@双语语料库#ai_product*]的构建以及[@机器学习#ai_tec*]方法的兴起，[@机器翻译#ai_tec*]方法逐渐由基于理性主义思维的规则方法转向基于经验主义思维的语料库方法。基于语料库的[$机器翻译#ai_tec*]方法又可以进一步划分为基于实例的翻译方法和基于统计模型的翻译方法。基于语料库的方法使用[@语料库#ai_product*]作为翻译知识的来源，无需人工编写规则，系统开发成本低，速度快；而且从[$语料库#ai_product*]中学习到的知识比较客观，覆盖性也比较好。但是这种系统性能严重依赖于[$语料库#ai_product*]，有着严重的数据稀疏问题，也不容易获得大颗粒度的高概括性知识。统计[$机器翻译#ai_tec*]（Statistics-based machine translation）的基本思想是充分利用[$机器学习#ai_tec*]技术，通过对大量的平行语料进行统计分析进行翻译。通俗来讲，源语到目的语的翻译过程是一个概率统计的问题，任何一个目的语句子都有可能是任何一个源语的译文，只是概率不同，[$机器翻译#ai_tec*]的任务就是找到概率最大的那个句子。[@20 世纪 90 年代初期#Date*]，[@IBM 的研究人员#Person*]ᨀ出了基于信源信道思想的统计[$机器翻译#ai_tec*]模型，并在实验中获得了初步的成功，正式标志着统计[$机器翻译#ai_tec*]时代的到来。不过由于当时计算机能力等方面限制，真正展开[$机器翻译#ai_tec*]方法研究的人并不多，统计[$机器翻译#ai_tec*]方法是否有效还受到人们的普遍怀疑，随着越来越多的人员投入到统计[$机器翻译#ai_tec*]中并取得成功，统计方法已经逐渐成为国际上[$机器翻译#ai_tec*]研究的主流方法之一。最初 [@IBM 研究人员#Person*]ᨀ出的是基于词的[$机器翻译#ai_tec*]模型，但是，由于这种[$机器翻译#ai_tec*]模型复杂度较高，翻译质量也不尽人意，因此逐渐被一些更加有效的翻译模型所替代。下图是当前[$机器翻译#ai_tec*]中一些典型的翻译模型。统计[$机器翻译#ai_tec*]（Statistics-based machine translation）的基本思想是充分利用[$机器学习#ai_tec*]技术，通过对大量的平行语料进行统计分析进行翻译。通俗来讲，源语到目的语的翻译过程是一个概率统计的问题，任何一个目的语句子都有可能是任何一个源语的译文，只是概率不同，[$机器翻译#ai_tec*]的任务就是找到概率最大的那个句子。[$20 世纪 90 年代初期#Date*]，[$IBM 的研究人员#Person*]ᨀ出了基于信源信道思想的统计[$机器翻译#ai_tec*]模型，并在实验中获得了初步的成功，正式标志着统计[$机器翻译#ai_tec*]时代的到来。不过由于当时计算机能力等方面限制，真正展开[$机器翻译#ai_tec*]方法研究的人并不多，统计[$机器翻译#ai_tec*]方法是否有效还受到人们的普遍怀疑，随着越来越多的人员投入到统计[$机器翻译#ai_tec*]中并取得成功，统计方法已经逐渐成为国际上[$机器翻译#ai_tec*]研究的主流方法之一。最初 [$IBM 研究人员#Person*]ᨀ出的是基于词的[$机器翻译#ai_tec*]模型，但是，由于这种[$机器翻译#ai_tec*]模型复杂度较高，翻译质量也不尽人意，因此逐渐被一些更加有效的翻译模型所替代。下图是当前[$机器翻译#ai_tec*]中一些典型的翻译模型。统计[$机器翻译#ai_tec*]也是基于[$语料库#ai_product*]的[$机器翻译#ai_tec*]方法，不需要人工撰写规则，而是从[$语料库#ai_product*]中获取翻译知识，这一点与基于实例的方法相同。为翻译建立统计模型，把翻译问题理解为搜索问题，即从所有可能的译文中选择概率最大的译文，基于实例的[$机器翻译#ai_tec*]则无需建立统计模型，二者的区别还在于，基于实例的[$机器翻译#ai_tec*]中，语言知识表现为实例本身，而统计[$机器翻译#ai_tec*]中，翻译知识表现为模型参数。统计[$机器翻译#ai_tec*]是以严格的数学理论做基础的。所有的翻译知识都是以概率的形式呈现，表现为某种参数。训练的过程就是为了得到这些参数，解码的过程则是利用这些参数去搜索匹配最好的译文，只要使用这些参数就不需要去搜索原始的[$语料库#ai_product*]。在整个过程中，[$机器翻译#ai_tec*]并不需要人工构造的翻译知识，所有的语言知识都是从[$语料库#ai_product*]中自动获取。统计[$机器翻译#ai_tec*]的成功在于采用了一种新的研究范式，这种研究范式已在语音识别等领域中被证明是一种成功的翻译，但在[$机器翻译#ai_tec*]中是首次使用。这种范式的明显特点是，公开的大规模的训练数据、周期性的公开测评和研讨以及开放源码工具。目前，统计[$机器翻译#ai_tec*]所使用的[$语料库#ai_product*]是双语句子对齐的[$语料库#ai_product*]，规模通常在几万句对到几百万句对不等。统计[$机器翻译#ai_tec*]的过程被看作是一个最优解搜索的过程，系统从巨大的可能译文中搜索最优的译文，搜索所使用的算法则采用[@人工智能#ai_tec*]中的一些成熟算法。统计翻译模型的发展，迄今为止经历了三个阶段。分别是基于词的模型，基于短语的模型和基于句子的模型。基于短语的模型中的“短语”表示连续的词串，该模型的基本思想是：首先从双语句子对齐的平行[$语料库#ai_product*]中抽取短语到短语的翻译规则，在翻译时将源语言句子切分为短语序列，利用翻译规则得到目标语言的短语序列，然后借助调序模型对目标语言短语序列进行排序，最终获得最佳的目标译文。其中，短语调序模型，尤其是长距离的短语调序，一直是短语翻译模型的关键问题。目前，基于短语的模型是最为成熟的模型，而基于句子的模型是当前研究的热点。统计[$机器翻译#ai_tec*]的模型可以表现为一个金字塔的形式。在这个金字塔上，越往塔尖的方向走，对语言的分析也越深入。理论上来说，对语言的分析越深入，所具有的排歧能力就应该越强，译文的质量也应该越高。但实际上，分析语言本身就是一个很难的问题，分析的深度越深，往往引入的错误也越多，反而会导致翻译质量的下降。因此，如何通过引入更深层的语言分析来ᨀ高模型的排歧能力，同时又要避免分析导致的错误，就成了统计翻译模型要解决的主要问题。统计[$机器翻译#ai_tec*]为自然语言翻译过程建立概率模型并利用平行[$语料库#ai_product*]训练模型参数，无需人工编写规则，利用[$语料库#ai_product*]直接训练得到[$机器翻译#ai_tec*]系统，人工成本低、开发周期短，只要有[$语料库#ai_product*]就很容易适应新的领域或者语种，成为 [@Google#Org*]、[@微软#Org*]、[@百度#Org*]等国内外公司[@在线翻译系统#ai_product*]的核心技术。尽管如此，统计[$机器翻译#ai_tec*]仍然面临着一些严峻的挑战。例如统计[$机器翻译#ai_tec*]依赖人类专家通过特征来表示各种翻译知识源，由于语言之间的结构转换非常复杂，人工设计特征难以保证覆盖所有的语言现象；统计[$机器翻译#ai_tec*]中的原规则结构复杂，对[$语料库#ai_product*]的依赖性强，引入复杂的语言知识比较困难，即使现在可以用大规模[$语料库#ai_product*]训练数据，但仍然面临着严重的数据稀疏问题。基于实例的翻译方法（Example-based Machine Translation）由[@日本#Gpe*][@翻译专家长尾真（Makoko Nagao）#Person*]ᨀ出，他在 [@1984 年#Date*]发表了[@《采用类比原则进行日-英机器翻译的一个框架》#MISC*]一文，探讨[$日本#Gpe*]人初学英语时翻译句子的基本过程，长尾真认为，[$日本#Gpe*]人初学英语时总是记住一些最基本的英语句子以及一些相对应的日语句子，他们要对比不同的英语句子和相对应的日语句子，并由此推论出句子的结构。参照这个学习过程，在[$机器翻译#ai_tec*]中，如果我们给出一些英语句子的实例以及相对应的日语句子，[$机器翻译#ai_tec*]系统可以通过识别和比较这些实例以及译文的相似之处和相差之处，从而挑选出正确的译文。在基于实例的[$机器翻译#ai_tec*]系统中，系统的主要知识源是双语对照的[@翻译实例库#ai_product*]，[@实例库#ai_product*]主要有两个字段，一个字段保存源语言句子，另一个字段保存与之对应的译文，每输入一个源语言的句子时，系统把这个句子同[$实例库#ai_product*]中的源语言句子字段进行比较，找出与这个句子最为相似的句子，并模拟与之相对应的译文，最后输出译文，这是一种由实例引导推理的[$机器翻译#ai_tec*]方法，整个翻译过程其实是查找和复现类似的例子，不需要对源语言进行任何分析，只需要通过类比，发现和记起特定的源语言表达或以前的翻译实例作为主要知识源来对新的句子进行翻译。基于实例的[$机器翻译#ai_tec*]系统中，翻译知识以实例和[@语义类词典#ai_product*]的形式表示，易于增加或删除，系统的维护简单易行，且利用了较大的[$翻译实例库#ai_product*]并进行精确地对比，有可能产生高质量译文，而且避免了基于规则的那些传统的[$机器翻译#ai_tec*]方法必须进行深层语言学分析的难点，在翻译策略上很有吸引力的。基于实例的[$机器翻译#ai_tec*]直接使用对齐的[$语料库#ai_product*]作为知识表示形式，知识库的扩充非常简单，而且不需要进行深层次的语言分析，也可以产生高质量的译文。但是基于实例的[$机器翻译#ai_tec*]系统的翻译质量取决于翻译记忆库的规模和覆盖率，至少要百万句对以上，因此如何构建大规模[@翻译记忆库#ai_product*]成为影响基于实例的[$机器翻译#ai_tec*]研究的关键。现阶段，由于缺少大规模的双语对齐[$语料库#ai_product*]，基于实例的[$机器翻译#ai_tec*]系统匹配率其实并不高，往往只有限定在特定的专业领域时，翻译效果才能达到使用要求。如果基于实例[$机器翻译#ai_tec*]匹配成功，可以获得相对较高质量的译文，因此基于实例的[$机器翻译#ai_tec*]一般和基于规则的[$机器翻译#ai_tec*]相结合使用，会产生比较好的翻译结构。对于匹配率过低的问题，可以试着做到短语级别的双语对齐，以ᨀ高匹配命中率，通过短语级别的局部匹配，结合相应的目标句子的框架，完成句子的翻译。从最初的基于规则的[$机器翻译#ai_tec*]到最新的依靠数据驱动进行的[$机器翻译#ai_tec*]，其总体发展趋势是要让计算机更加自主的学习如何翻译。利用平行[$语料库#ai_product*]进行数据的训练，是ᨀ高[$机器翻译#ai_tec*]准确性和可读性的关键，深度学习的引入则成了当前热点。[@1943 年#Date*][@卡洛可和皮茨#Person*]ᨀ出了抽象的[@神经元模型 MP#ai_tec*]，该模型可以看作深度学习的雏形。[@1957 年#Date*] [@Frank Rosenblatt#Person*] 发明了[@感知机#ai_product*]，是当时首个可以学习的[@人工神经网络#ai_tec*]。[@1969 年#Date*][@Marvin Minksy 和 Seymour Papert #Person*]用详细的数学证明了[$感知机#ai_product*]的弱点，神经网络研究进入冰河期。[$1984 年#Date*][@福岛邦彦#Person*]ᨀ出了卷积神经网络的原始模型神经[$感知机#ai_product*]，产生了卷积和池化的思想。1986年Hinton等人ᨀ出一般Delta法则，并用[@反向传播训练MLP#ai_tec*]。[@1998年#Date*]以[@Yann LeCun#Person*]为首的[@研究人员实现#Person*]了 [@5 层的卷积神经网络——LeNet-5#ai_tec*]，以识别手写数字。LeNet-5 标志着CNN（卷积神经网络）的真正面世，LeNet-5 的ᨀ出把 CNN 推上了一个小高潮。之后 [@SVM#ai_product*] 兴起。[@2012 年#Date*] [@AlexNet #Person*]在 [@ImageNet#MISC*] 上夺冠，掀起了[@深度学习#ai_tec*]的热潮。[@AlexNet#ai_tec*]可以算是 [@LeNet #ai_tec*]的一种更深更宽的版本，并加上了 relu、dropout 等技巧。这条思路被后人发展，出现了 [@VGG#ai_tec*]，[@GoogLeNet #ai_tec*]等网络。[@2016 年#Date*][@何恺明#Person*]在层次之间加入跳跃连接，[@Resnet#ai_tec*]极大增加了网络深度，效果有很大ᨀ升。[@cvpr best paper densenet #ai_program*]也是沿着这条思路发展的。除此之外，[@cv 领域的特定任务#ai_program*]还出现了各种各样的模型（Mask-RCNN 等），这里不一一介绍。[@2017 年#Date*] [@Hinton #Person*]认为反省传播和传统神经网络有缺陷，继而ᨀ出了 [@Capsule Net#ai_tec*]。但是目前在 cifar 等数据集上效果一般，这条思路还需要继续验证和发展。传统的生成模型是要预测联合概率分布 P（x，y）。[@RBM #ai_tec*]本在 [@1986 年#Date*]的时候就存在，只是 [@2006 年#Date*]重新作为一个生成模型，并且堆叠成为 [@deep belief network#ai_tec*]，使用逐层贪婪或者wake-sleep 的方法训练，[$Hinton #Person*]等人从此开始使用[$深度学习#ai_tec*]重新包装神经网络。[@Auto-Encoder#ai_tec*] ᨀ出于[@上世纪 80 年代#Date*]，现在随着计算能力的进步重新登上舞台。[@2008 年#Date*]，Bengio 等人又ᨀ出 [@denoise Auto-Encoder#ai_tec*]。[@Max Welling #Person*]等人使用神经网络训练 Variationalauto-encoder。此模型可以通过隐变量的分布采样，经过后面的 [@decoder 网络#ai_tec*]直接生成样本。[@GAN（生成对抗网络）#ai_tec*]于[@ 2014 年#Date*]ᨀ出。它是一个生成模型，通过判别器 D 和生成器 G的对抗训练，直接使用神经网络 G 隐式建模样本整体的概率分布。每次运行便相当于从分布中采样。[@DCGAN #ai_tec*]是较好的卷积神经网络实现，而 WGAN 则是通过维尔斯特拉斯距离替换原来的 JS 散度来度量分布之间的相似性工作，训练更稳定。[@PGGAN #ai_tec*]则逐层增大网络，生成极其逼真的人脸。[@1982 年#Date*]出现的 [@Hopfield Network #ai_tec*]有了递归网络的思想。[@1997 年#Date*] [@Schmidhuber #Person*]发明[@ LSTM#ai_tec*]，并做了一系列的工作。但是更有影响力的还是 [@2013 年#Date*]由 [$Hinton #Person*]组使用 [@RNN#ai_tec*] 做的[@语音识别工作#ai_product*]。文本方面，Bengio ᨀ出了一种基于神经网络的语言模型，后来 [$Google#Org*] ᨀ出 word2vec也包含了一些反向传播的思想。在[$机器翻译#ai_tec*]等任务上，逐渐出现了以 [$RNN#ai_tec*] 为基础的 seq2seq模型，模型通过编码器把一句话的语义信息压成向量再通过解码器输出，但更多的还要和注意力模型结合。之后以字符为单位的 CNN 模型在很多语言任务也表现不俗，而且时空消耗更少。LSTM/RNN 模型中的注意力机制是用于克服传统编码器-解码器结构存在的问题的。其中，自注意力机制实际上就是采取一种结构令其同时考虑同一序列局部和全局的信息。该领域最出名的是 [@DeepMind#ai_tec*]，这里列出的 [@David Silver #Person*]则是一直研究 [@reinforcementlearning（rl，强化学习）#ai_tec*]的高管。q-learning 是很有名的传统 rl 算法，deep q-learning 则是将原来的 q 值表用神经网络代替。之后 [$David Silver #Person*]等人又利用其测试了许多游戏，发在了 Nature 上。增强学习在 double duel的进展，主要是 q-learning 的权重更新时序。[$DeepMind#ai_tec*] 的其他工作诸如 [@DDPG#ai_tec*]、[@A3C #ai_tec*]也非常有名，它们是基于 policy gradient 和神经网络结合的变种。可以说基于深度预习的[$机器翻译#ai_tec*]，显著地ᨀ升了[$机器翻译#ai_tec*]的质量，接近普通人的水平，是当前[$机器翻译#ai_tec*]领域的热点。大致可以分为两种情况，一是领用[$深度学习#ai_tec*]改进统计[$机器翻译#ai_tec*]中的相关模块；二是直接利用神经网络实现源语言到目标语言的映射，即端到端的神经[$机器翻译#ai_tec*]。[@美国 #Gpe*][@BBN 公司#Org*]的研究人员进一步ᨀ出[@神经网络联合模型（Neural Network Joint Models）#ai_product*]。他们ᨀ出，对于决定当前词来说，不仅仅是目标语言端的历史信息有着重要的作用，源语言端的相关部分也起着很重要的作用。这种观点是对传统的语言模型的一种颠覆，因为传统的语言模型往往只考虑目标语言端的前 n-1 个词，而不会探索更多。使用分布式表示能够缓解数据稀疏问题，再加上神经网络联合模型能够使用丰富的上下文信息，融入了[$深度学习#ai_tec*]的翻译方法相对应传统的翻译方法有了一个质的进步。使用神经网络对[$机器翻译#ai_tec*]来说，还有一个更为显著的好处，即能够解决特征难以涉及的问题。例如调序模型。基于短语的统计[$机器翻译#ai_tec*]的重要调序方法之一是基于反向转录文法的调序模型。这种模型将调序视作一个二元分类问题，即两个相邻源语言词串的译文顺序有顺序拼接和逆序拼接两种处理方法。传统方法通常使用最大熵分类器，但是这一方法面临着一个重要难点，即如何设计能够捕获调序规律的特征。要从众多的词语集合中选出能够对调序决策起到关键作用的词语是非常困难的，而且词串的长度一般都非常长，更是为此增加了难度。所以，基于反向转录文法的调序模型由于无法把握基于词串的特征设计问题，从而无法充分利用整个词串的信息，造成了信息的白白流失。利用神经网络能够较好的缓解特征设计的问题，首先词串的分布式表示可以利用[@递归自动编码器#ai_product*]生成，然后神经网络分类器可以基于四个词串的分布式表示来建立。因此，基于神经网络的调序模型不需要人工参与设计特征就能够自主利用整个词串的信息，调序分类准确率和翻译质量显著ᨀ高。实际上，[$深度学习#ai_tec*]不仅停留在为[$机器翻译#ai_tec*]生成新的特征这一步，更能够将现有的特征集合转化生成新的特征集合，[@翻译模型#ai_product*]的表达能力有了显著ᨀ升。端到端神经[$机器翻译#ai_tec*]（End-to-End Neural Machine Translation）是一种全新的[$机器翻译#ai_tec*]方法，于 [$2013 年#Date*]兴起。这种翻译方法通过神经网络直接将源语言文本映射成目标语言文本。这种方法仅通过非线性的神经网络便能直接实现自然语言文本的转换，不再需要由人工设计词语对齐、短语切分、句法树等隐结构，也不需要人工设计特征。[$2013 年#Date*]，[@英国#Gpe*][@牛津大学#Org*]的 [@Nal Kalchbrenner #Person*]和 [@Phil Blunsom#Person*] ᨀ出了端到端的神经[$机器翻译#ai_tec*]，他们ᨀ出了一个新框架，即“[@编码-解码#ai_product*]”的框架。对于一个源语言句子，首先将它映射为一个连续、稠密的向量，这一过程是通过编码器实现的，然后再将这个向量转化为目标语言的句子，这一过程通过[@解码器#ai_product*]实现。[@Kalchbrenner #Person*]和 [@Blunsom #Person*]在论文中使用的编码器是卷积神经网络（Convolutional Neural Network），[$解码器#ai_product*]是递归神经网络（Recurrent NeuralNetwork）。能够捕获全部历史信息和处理变长字符串是递归神经网络的优点。这种新的架构，统计[$机器翻译#ai_tec*]的线性模型被非线性模型取代，隐结构流水线被单个复杂的神经网络取代，语义等价性通过连接编码器和[$解码器#ai_product*]的向量来᧿述，同时，递归神经网络可以捕获无限长的历史信息。理论上，端到端的神经[$机器翻译#ai_tec*]能够捕获无限长的历史信息，可以取得理想的翻译效果，但是在真正处理长距离的依赖关系时还是有困难的。为此，[@2014 年#Date*]，[@长短期记忆（Long short-Term Memory）#MISC*]被[$美国 #Gpe*][$Google#Org*] 公司引入端到端的神经[$机器翻译#ai_tec*]。通过设置门开关的方法长短期记忆能够较好的捕获长距离依赖。由于递归神经网络无论在[@编码器#ai_product*]还是[$解码器#ai_product*]里的使用，都使得端到端神经[$机器翻译#ai_tec*]性能得到了ᨀ升，取得了与传统[$机器翻译#ai_tec*]相当甚至更好地翻译效果。但是，要想实现准确的编码和语言句子，[$编码器#ai_product*]都需要将它映射为一个维度固定的向量，这是一个极大的挑战。[@Yoshua Bengio 研究组#Org*]针对这一问题ᨀ出了基于注意力（Attention）的端到端神经网络翻译。其基本思想就是，在[$解码器#ai_product*]生成单个的目标语言，有相关性的其实仅仅是小部分的源语言词，绝大多数源语言词都是无关的，这样就不需要使用整个源语言句子的向量，只要使用每个目标语言词相关的源语言端的上下文向量即可。为此，他们ᨀ出了一整套基于内容的注意力计算方法，这套方法能够更好地处理长距离依赖，同时ᨀ升端到端神经[$机器翻译#ai_tec*]的准确率和质量。虽然端到端的神经[$机器翻译#ai_tec*]近年来发展迅速，但仍然存在很大的ᨀ升空间。首先是可解释性差。有别于传统的[$机器翻译#ai_tec*]在设计模型时根据语言学知识进行架构的方式，神经网络内部全部使用向量表示，从语言学的角度看可解释性很差，在设计新结构时如何融入语言学知识成为新的挑战。其次训练复杂度高，端到端神经[$机器翻译#ai_tec*]的训练复杂度是传统统计[$机器翻译#ai_tec*]不可比的，对于计算资源的依赖程度和要求也更高，想要获得较理想的实验周期必须使用较大规模的 GPU，因此计算资源成为端到端神经[$机器翻译#ai_tec*]的一个重要问题。