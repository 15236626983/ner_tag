情感分类的先河。紧接着，随着1985年Minsky教授“让计算机具有情感能力”观点的提出，以及人工智能领域的研究者们对情感智能重要性认识的日益加深，越来越多的科研机构开始了语音情感识别研究的探索。在20世纪80年代末至90年代初期，麻省理工学院多媒体实验室构造了一个“情感编辑器”对外界各种情感信号进行采集，综合使用人体的生理信号、面部表情信号、语音信号来初步识别各种情感，并让机器对各种情感做出适当的简单反应；1999年，Moriyama提出语音和情感之间的线性关联模型，并据此在电子商务系统中建造出能够识别用户情感的图像采集系统语音界面，实现了语音情感在电子商务中的初步应用。整体而言，语音情感识别研究在该时期仍旧处于初级阶段，主要侧重于情感的声学特征分析这一方面，作为研究对象的情感语音样本也多表现为规模小、自然度低、语义简单等特点，虽然有相当数量的有价值的研究成果相继发表，但是并没有形成一套被广泛认可的、系统的理论和研究方法。进入21世纪以来，随着计算机多媒体信息处理技术等研究领域的出现以及人工智能领域的快速发展，语音情感识别研究被赋予了更多的迫切要求，发展步伐逐步加快。2000年，在爱尔兰召开的ISCA Workshop on Speech and Emotion国际会议首次把致力于情感和语音研究的学者聚集在一起。近10余年来，语音情感识别研究工作在情感描述模型的引入、情感语音库的构建、情感特征分析等领域的各个方面都得到了发展。下面将从语音情感数据库的采集、语音情感标注以及情感声学特征分析方面介绍语音情感计算。 . 语音情感数据库的采集 语音情感识别研究的开展离不开情感语音数据库的支撑。情感语音库的质量高低，直接决定了由它训练得到的情感识别系统的性能好坏。评价一个语音情感数据库好坏的一个重要标准是数据库中语音情感是否具备真实的表露性和自发性。目前，依据语音情感激发类型的不同，语音情感数据库可分为表演型、诱发型和自发型三种。 具体来说，表演型情感数据库通过专业演员的表演，把不同情感表达出来。在语音情感识别研究初期，这一采集标准被认为是研究语音情感识别比较可靠的数据来源，因为专业演员在表达情感时，可以通过专业表达获得人所共知的情感特征。比如，愤怒情感的语音一般会具有很大的幅值和强度，而悲伤情感的语音则反之。由于这一类型的数据库具有表演的性质，情感的表达会比真实情感夸大一点，因此情感不具有自发的特点。依据该类型数据库来学习的语音情感识别算法，不一定能有效应用于真实生活场景中。第二种称之为诱发型情感数据库。被试者处于某一特定的环境，如实验室中，通过观看电影或进行计算机游戏等方式，诱发被试者的某种情感。目前大部分的情感数据库都是基于诱发的方式建立的。诱发型情感数据库产生的情感方式相较于表演型情感数据库，其情感特征更具有真实性。最后一种类型属于完全自发的语音情感数据库，其语料采集于电话会议、电影或者电话的视频片段，或者广播中的新闻片段等等。由于这种类型的语音情感数据最具有完全的真实性和自发性，应该说最适合用于实用的语音情感识别。但是，由于这些语音数据涉及道德和版权因素，妨碍了它在实际语音情感识别中的应用。  . 语音情感数据库的标注 对于采集好的语音情感库，为了进行语音情感识别算法研究，还需要对情感语料进行标注。标注方法有两种类型： 离散型情感标注法指的是标注为如生气、高兴、悲伤、害怕、惊奇、讨厌和中性等，这种标注的依据是心理学的基本情感理论。基本情感论认为，人复杂的情感是由若干种有限的基本情感构成的，就像我们自古就有“喜、怒、哀、乐，恐、悲、惊”七情的说法。不同的心理学家对基本情感有不同的定义，由此可见，在心理学领域对基本情感类别的定义还没有一个统一的结论，因此不同的语音情感数据库包含的情感类别也不尽相同。这不利于在不同的语音情感数据库上，对同一语音情感识别算法的性能进行评价。此外，众所周知，实际生活中情感的类别远远不止有限几类。基于离散型情感标注法的语音情感识别容易满足多数场合的需要，但无法处理人类情感表达具有连续性和动态变化性的情况。在实际生活中，普遍存在着情感变化的语音，比如前半句包含了某一种情感，而后半句却包含了另外一种情感，甚至可能相反。例如，某人说话时刚开始很高兴，突然受到外界刺激，一下子就生气了。对于这种在情感表达上具有连续和动态变化的语音，采用离散型情感标注法来进行语音情感识别就不合适了。因为此时语音的情感，己不再完全属于某一种具体的情感。 维度情感空间论基于离散型情感标注法的缺陷，心理学家们又提出了维度情感空间论，即对情感的变化用连续的数值进行表示。不同研究者所定义的情感维度空间数目有所不同，如二维、三维甚至四维模型。针对语音情感，最广为接受和得到较多应用的为二维连续情感空间模型，即“激活维－效价维”（Arousal-Valence）的维度模型。“激活维”反映的是说话者生理上的激励程度或者采取某种行动所作的准备，是主动的还是被动的；“效价维”反映的是说话者对某一事物正面的或负面的评价。随着多模态情感识别算法的研究，为了更细致的地描述情感的变化，研究者在“激活维－效价维”（Arousal-Valence）二维连续情感空间模型的基础上，引入“控制维”，即在“激活维－效价维－控制维（Arousal-Valence/Pleasure-Power/Dominance）”三维连续情感空间模型上对语音情感进行标注和情感计算。需要强调的是，离散型和连续型情感标注之间，它们并不是孤立的，而是可以通过一定映射进行相互转换。 . 情感声学特征分析 情感声学特征分析主要包括声学特征提取和声学特征选择、声学特征降维。采用何种有效的语音情感特征参数用于情感识别，是语音情感识别研究最关键的问题之一，因为所用的情感特征参数的优劣直接决定情感最终识别结果的好坏[9] 。 声学特征提取。目前经常提取的语音情感声学特征参数主要有三种：韵律特征、音质特征以及谱特征。在早期的语音情感识别研究文献中，针对情感识别所首选的声学特征参数是韵律特征，如基音频率、振幅、发音持续时间、语速等。这些韵律特征能够体现说话人的部分情感信息，较大程度上能区分不同的情感。因此，韵律特征已成为当前语音情感识别中使用最广泛并且必不可少的一种声学特征参数除了韵律特征，另外一种常用的声学特征参数是与发音方式相关的音质特征参数。三维情感空间模型中的“激发维”上比较接近的情感类型，如生气和高兴，仅使用韵律特征来识别是不够的。音质特征包括共振峰、频谱能量分布、谐波噪声比等，不仅能够很好地表达三维中的“效价维”信息，而且也能够部分反映三维中的“控制维”信息。因此，为了更好地识别情感，同时提取韵律特征和音质特征两方面的参数用于情感识别，已成为语音情感识别领域声学特征提取的一个主要方向。谱特征参数是一种能够反映语音信号的短时功率谱特性的声学特征参数，Mel频率倒谱系数（Mel-scale Frequency Cepstral Coefficients，MFCC）是最具代表性的谱特征参数，被广泛应用于语音情感识别。由于谱特征参数及其导数，仅反映语音信号的短时特性，忽略了对情感识别有用的语音信号的全局动态信息。近年来，为了克服谱特征参数的这种不足之处，研究者提出了一些改进的谱特征参数，如类层次的谱特征、调制的谱特征和基于共振峰位置的加权谱特征等。 声学特征选择。为了尽量保留对情感识别有意义的信息，研究者通常都提取了较多的与情感表达相关的不同类型的特征参数，如韵律特征、音质特征、谱特征等。任意类型特征都有各自的侧重点和适用范围，不同的特征之间也具有一定的互补性、相关性。此外，这些大量提取的特征参数直接构成了一个高维空间的特征向量。这种高维性质的特征空间，不仅包含冗余的特征信息，导致用于情感识别的分类器训练和测试需要付出高昂的计算代价，而且情感识别的性能也不尽如人意。因此，非常有必要对声学特征参数进行特征选择或特征降维处理，以便获取最佳的特征子集，降低分类系统的复杂性和提高情感识别的性能。 特征选择是指从一组给定的特征集中，按照某一准则选择出一组具有良好区分特性的特征子集。特征选择方法主要有两种类型：封装式（Wrapper）和过滤式（Filter）。Wrapper算法是将后续采用的分类算法的结果作为特征子集评价准则的一部分，根据算法生成规则的分类精度选择特征子集。Filter 算法是将特征选择作为一个预处理过程，直接利用数据的内在特性对选取的特征子集进行评价，独立于分类算法。 声学特征降维。特征降维是指通过映射或变换方式将高维特征空间映射到低维特征空间，已达到降维的目的。特征降维算法分为线性和非线性两种。最具代表性的两种线性降维算法，如主成分分析PCA（Principal Component Analysis）和线性判别分析 LDA（Linear Discriminant Analysis），已经被广泛用于对语音情感特征参数的线性降维处理。也就是，PCA和LDA方法被用来对提取的高维情感声学特征数据进行嵌入到一个低维特征子空间，然后在这降维后的低维子空间实现情感识别，提高情感识别性能。近年来，新发展起来的基于人类认知机理的流形学习方法比传统的线性PCA和LDA方法更能体现事物的本质，更适合于处理呈非线性流形结构的语音情感特征数据。但这些原始的流形学习方法直接应用于语音情感识别中的特征降维，所取得的性能并不令人满意。主要原因是他们都属于非监督式学习方法，没有考虑对分类有帮助的已经样本数据的类别信息。尽管流形学习方法能够较好地处理非线性流形结构的语音特征数据，但是流形学习方法的性能容易受到其参数如邻域数的影响，而如何确定其最佳的邻域数，至今还缺乏理论指导，一般都是根据样本数据的多次试验结果来粗略地确定。因此，对于流形学习方法的使用，如何确定其最佳参数，还有待深入研究。 下表为常用语音情感数据库，供读者参考： 表 2 常用的语音情感数据库[10]  数据库名称  简介  Belfast英语情感数据库  Belfast情感数据库由Queen大学的Cowie和Cowie录制，由40位录音人（18岁~69岁，20男20女）对5个段落进行演绎得到.每个段落包含7~8个句子，且具有某种特定的情感倾向，分别为生气/anger、悲伤/sadness、高兴/happiness、恐惧/fear、中性/neutral。  柏林EMO-DB德语情感语音库  DMO-DB是由柏林工业大学录制的德语情感语音库，由10位演员（5男5女）对10个语句（5长5短）进行7种情感（中性/neutral、生气/anger、害怕/fear、高兴/joy、悲伤/sadness、厌恶/disgust、无聊/boredom）的模拟得到，共包含800句语料，采样率48kHz（后压缩到16kHz），16bit量化。语料文本的选取遵从语义中性、无情感倾向的原则，且为日常口语化风格，无过多的书面语修饰语音的录制在专业录音室中完成。经过20个参与者（10男10女）的听辨实验，得到84.3%的听辨识别率。  FAU AIBO儿童德语情感语音库  FAU AIBO录制了51名儿童（10岁~13岁，21男30女）在与索尼公司生产的电子宠物AIBO游戏过程中的自然语音，并且只保留了情感信息明显的语料，总时长为9.2小时（不包括停顿），包括48401个单词，由DAT-recorder录制，48kHz采样（而后压缩到16kHz），16bit量化。为了记录真实情感的语音，工作人员让孩子们相信AIBO能够对他们的口头命令加以反应和执行，而实际上，AIBO则是由工作人员暗中人为操控的。标注共涵盖包括joyful、irritated、angry、neutral等在内的11个情感标签。该数据库中的18216个单词被选定为INTERSPEECH 2009年情感识别竞赛用数据库。  CASIA汉语情感语料库  该数据库由中国科学院自动化研究所录制，由4位录音人（2男2女）在纯净录音环境下（信噪比约为35db）分别在5类不同情感下（高兴、悲哀、生气、惊吓、中性）对500句文本进行的演绎得到，16kHz采样，16bit量化，经过听辨筛选，最终保留其中9600句。  ACCorpus系列汉语情感数据库  该系列情感数据库由清华大学和中国科学院心理研究所合作录制，包含5个相关子库： 1）ACCorpus_MM多模态、多通道的情感数据库；  2）ACCorpus_SR情感语音识别数据库； 3）ACCorpus_SA汉语普通话情感分析数据库； 4）ACCorpus_FV人脸表情视频数据库； 5）ACCorpus_FI人脸表情图像数据库。 其中，ACCorpus_SR子库共由50位录音人（25男25女）对5类情感（中性、高兴、生气、恐惧和悲伤）演绎得到，16kHz采样，16bit量化。每个发音者的数据均包含语音情感段落和语音情感命令两种类型。  VAM数据库  VAM数据库是一个以科学研究为目的的无偿数据库，通过对一个德语电视谈话节目“Vera am美国麻省理工学院tag”的现场录制得到，数据库包含语料库、视频库、表情库这3个部分。谈话内容均为无脚本限制、无情绪引导的纯自然交流。以VAM-audio库为例，该子库包含来自47位节目嘉宾的录音数据947句，wav格式，16kHz采样，16bit量化。所有数据以句子为单位进行保存（1018句），标注在Valence、Activation和Dominance这3个情感维度上进行，标注值处于-1~1之间，最终的情感值是相关标注者的平均值。  Semaine数据库  Semaine数据库是一个面向自然人机交互和人工智能研究的数据库（http:// semaine-db.eu/）。数据录制在人机交互的场景下进行，20个用户（22岁~60岁，8男12女）被要求与性格迥异的4个机器角色进行交谈（实际上，机器角色由工作人员扮演）。这4个角色分别是： 1）温和而智慧的Prudence： 2）快乐而外向的Poppy； 3）怒气冲冲的Spike； 4）悲伤而抑郁的Obadiah。 录音过程在专业配置录音室内进行，同时有5个高分辨率、高帧频摄像机和4个麦克风进行数据的收集，其中，音频属性为48kHz采样，24bit量化，数据时长在7小时左右。标注工作由多个参与者借助标注工具FEELTRACE在Valence、Activation、Power、Expectation和Intensity这5个情感维度上进行。该数据库中的部分数据被用于AVEC 2012的竞赛数据库。  Maribor数据库  该数据库由斯洛文尼亚Maribor大学录制，包含8类情感语料（厌恶/disgust、惊奇/surprise、高兴/joy、恐惧/fear、生气/anger、悲伤/sadness、快高中性/fast loud neutral、低慢中性/low soft neutral)，通过4种语言（英语、斯洛文尼亚语、法语、西班牙语）演绎，每类情感各有186句语料。  Kids’ Audio Speech Corpus NSF/ITR Reading Project  该数据库由科罗拉多大学的Cole教授及其助理录制，旨在从儿童那里收集足够的音频和视频数据，以便实现听觉和视觉识别系统的发展，使与电子教师面对面的对话互动成为可能。  Emotional Prosody Speech and Transcripts  该数据库由宾夕法尼亚大学录制，包含15类情感语料，例如s hot anger, cold anger, panic-anxiety, despair, sadness, elation, happiness, interest, boredom, shame, pride, disgust and contempt  2.1.3 视觉情感计算 表情作为人类情感表达的主要方式，其中蕴含了大量有关内心情感变化的信息，通过面部表情可以推断内心微妙的情感状态。但是让计算机读懂人类面部表情并非简单的事情。人脸表情识别是人类视觉最杰出的能力之一。而计算机进行自动人脸表情识别所利用的主要也是视觉数据。无论在识别准确性、速度、可靠性还是稳健性方面，人类自身的人脸表情识别能力都远远高于基于计算机的自动人脸表情识别。因此，自动人脸表情识别研究的进展一方面依赖计算机视觉、模式识别、人工智能等学科的发展，另一方面还依赖对人类本身识别系统的认识程度，特别是对人的视觉系统的认识程度。 早在20世纪70年代，关于人脸表情识别的研究就已经展开，但是早期主要集中在心理学和生物学方面。随着计算机技术的发展，人脸表情识别技术逐渐发展起来，至上世纪90年代，该领域的研究已经非常活跃。大量文献显示表情识别与情感分析已从原来的二维图像走向了三维数据研究，从静态图像识别研究专项实时视频跟踪。下面将从视觉情感信号获取、情感信号识别以及情感理解与表达方面介绍视觉情感计算。 . 视觉情感信号获取 表情参数的获取，多以二维静态或序列图像为对象，对微笑的表情变化难以判断，导致情感表达的表现力难以提高，同时无法体现人的个性化特征，这也是表情识别中的一大难点。以目前的技术，在不同的光照条件和不同头部姿态下，也不能取得满意的参数提取效果。 由于三维图像比二维图像包含更多的信息量，可以提供鲁棒性更强，与光照条件和人的头部姿态无关的信息，用于人脸表情识别的特征提取工作更容易进行。因此，目前最新的研究大多利用多元图像数据来进行细微表情参数的捕获。该方法综合利用三维深度图像和二维彩色图像，通过对特征区深度特征和纹理彩色特征的分析和融合，提取细微表情特征，并建立人脸的三维模型，以及细微表情变化的描述机制。 . 视觉情感信号识别 视觉情感信号的识别和分析主要分为面部表情的识别和手势识别两类： 对于面部表情的识别，要求计算机具有类似于第三方观察者一样的情感识别能力。由于面部表情是最容易控制的一种，所以识别出来的并不一定是真正的情感，但是，也正由于它是可视的，所以它非常重要，并能通过观察它来了解一个人试图表达的东西。到目前为止，面部表情识别模型都是将情感视为离散的，即将面部表情分成为数不多的类别，例如“高兴”、“悲伤”、“愤怒”等。1971年，Ekman和Friesen研究了6种基本表情（高兴、悲伤、惊讶、恐惧、愤怒和厌恶），并系统地建立了上千幅不同的人脸表情图像库。六种基本表情的具体面部表现如表3所示。1978年，他们研究了情感类别之间的内在关系，开发了面部动作编码系统（FACS）。系统描述了基本情感以及对应的产生这种情感的肌肉移动的动作单元。他们根据人脸的解剖学特点，将其划分成大约46个既相互独立又相互联系的运动单元（AU），并分析了这些运动单元的运动特征及其所控制的主要区域以及与之相关的表情，给出了大量的照片说明。面部识别器一般要花五分钟来处理一种面部表情，准确率达到98％。 马里兰大学的Yeser Yacoob和Larry Davis提出了另一种面部表情识别模型，它也是基于动作能量模版，但是将模版、子模版（例如嘴部区域）和一些规则结合起来表达情感。例如，愤怒的表情在从眼睛区域提取的子模版中，特别是眉毛内敛、下垂，在嘴巴区域子模版中，特别是嘴巴紧闭，两个子模板结合起来，就很好表达了愤怒这一情感。后续的研究总体上结合生物识别方法及计算机视觉进行，依据人脸特定的生物特征，将各种表情同脸部运动细节（几何网格的变化）联系起来，收集样本，提取特征，构建分类器。但是目前公开的用于表情识别研究的人脸图像数据库多是采集志愿者刻意表现出的各种表情的图像，与真实情形有出入。 表 3 脸部表情运动特征具体表现 表情  额头