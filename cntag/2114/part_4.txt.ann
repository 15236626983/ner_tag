、眉毛  眼睛  脸的下半部  惊奇  1） 眉毛抬起，变高变弯 2） 眉毛下的皮肤被拉伸 3） 皱纹可能横跨额头  1） 眼睛睁大，上眼皮抬高，下眼皮下落 2） 眼白可能在瞳孔的上边和/或下边露出来  下颌下落，嘴张开，唇和齿分开，但嘴部不紧张，也不拉伸  恐惧  1） 眉毛抬起并皱在一起 2） 额头的皱纹只集中在中部，而不横跨整个额头  上眼睑抬起，下眼皮拉近  嘴张，嘴唇或轻微紧张，向后拉；或拉长，同时向后拉  厌恶  眉毛压低，并压低上眼睑  在下眼皮下部出现横纹，脸颊推动其向上，并不紧张  1） 上唇抬起 2） 下唇与上唇紧闭，推动上唇向上，嘴角下拉，嘴唇微凸起 3） 鼻子皱起 4） 脸颊抬起  愤怒  1） 眉毛皱在一起，压低 2） 在眉宇间出现竖直皱纹  1） 下眼皮拉紧，抬起或不太起 2） 上眼皮拉紧，眉毛压低 3） 眼睛瞪大，可能鼓起  1） 唇有两种基本的位置：紧闭，嘴角拉直或向下，张开，仿佛要喊 2） 鼻孔可能张大  高兴  眉毛参考：稍微下弯  1） 下眼睑下边可能有皱纹，可能鼓起，但并不紧张 2） 鱼尾纹从外眼角向外扩张  1） 唇角向后拉并抬高 2） 嘴可能被张大，牙齿可能露出 3） 一道皱纹从鼻子一直延伸到嘴角外部 4） 脸颊被抬起  悲伤  眉毛内角皱在一起，抬高，带动眉毛下的皮肤  眼内角的上眼皮抬高  1） 嘴角下拉 2） 嘴角可能颤抖  对于手势识别来说，一个完整的手势识别系统包括三个部分和三个过程。三个部分分别是：采集部分、分类部分和识别部分；三个过程分别是：分割过程、跟踪过程和识别过程。采集部分包括了摄像头、采集卡和内存部分。在多目的手势识别中，摄像头以一定的关系分布在用户前方。在单目的情况下，摄像头所在的平面应该和用户的手部运动所在的平面基本水平。分类部分包括了要处理的分类器和结果反馈回来的接收比较器。用来对之前的识别结果进行校正。识别部分包括了语法对应单位和相应的跟踪机制，通过分类得到的手部形状通过这里一一对应确定的语义和控制命令。分割过程包括了对得到的实时视频图像进行逐帧的手部分割，首先得到需要关注的区域，其次在对得到的区域进行细致分割，直到得到所需要的手指和手掌的形状。跟踪过程包括对手部的不断定位和跟踪，并估计下一帧手的位置。识别过程通过对之前的知识确定手势的意义，并做出相应的反应，例如显示出对应的手势或者做出相应的动作，并对不能识别的手势进行处理，或者报警或者记录下特征后在交互情况下得到用户的指导。手势识别的基本框架如下图所示：  图 2 手势识别的基本框架 . 情感理解与表达 通过对情感的获取、分析与识别，计算机便可了解其所处的情感状态。[@情感计算#ai_tec*]技术的实质是通过各种传感器获取由人的情感所引起的表情生理变化信号，利用“[@情感模型#ai_tec*]”对这些信号进行识别和分析，从而理解人的情感并做出适当的响应。因此，这部分主要研究的是如何根据情感信息的识别结果，对用户的情感变化做出最适宜的反应。随着情感信息捕获技术的提高和情感数据资源的扩大，对多特征融合的[@情感理解模型#ai_tec*]的研究能力将会有进一步的提高和突破。前面的研究是从视觉面部表情或者是行为特征来推断情感状态。[@情感表达#ai_tec*]则是研究其反过程，即给定某一情感状态, 研究如何使这一情感状态在一种或几种视觉面部表情或行为特征中体现出来。例如，如何在手势合成和面部表情合成中得以体现，使机器具有情感，能够与用户进行情感交流[12] 。 下表为常用视觉情感数据库，供读者参考： 表 4 常用的表情分析数据库 数据库  简介  地址  KDEF与AKDEF（karolinska directed emotional faces）数据集  这个数据集最初是被开发用于心理和医学研究目的。它主要用于知觉，注意，情绪，记忆等实验。在创建数据集的过程中，特意使用比较均匀，柔和的光照，被采集者身穿统一的T恤颜色。这个数据集，包含70个人，35个男性，35个女性，年龄在20至30岁之间。没有胡须，耳环或眼镜，且没有明显的化妆。7种不同的表情，每个表情有5个角度。总共4900张彩色图。尺寸为562*762像素。  https://link.zhihu.com/?target=http%3A//www.emotionlab.se/kdef/  RaFD数据集  该数据集是Radboud大学Nijmegen行为科学研究所整理的，这是一个高质量的脸部数据库，总共包含67个模特：20名白人男性成年人，19名白人女性成年人，4个白人男孩，6个白人女孩，18名摩洛哥男性成年人。总共8040张图，包含8种表情，即愤怒，厌恶，恐惧，快乐，悲伤，惊奇，蔑视和中立。每一个表情，包含3个不同的注视方向，且使用5个相机从不同的角度同时拍摄的。  http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main  Fer2013数据集  图片的分辨率比较低，共6种表情。分别为0 anger生气、1 disgust厌恶、2 fear恐惧、3happy开心、4 sad伤心、5 surprised惊讶、6 normal中性。  https://github.com/npinto/fer2013  CelebFaces Attributes Dataset （CelebA）数据集  该数据集是商汤科技的一个用于研究人脸属性的数据集，一个包含超过200K名人图像的大型人脸属性数据集，每个数据集都有40个属性注释。该数据集中的图像涵盖了大型姿态变化和复杂背景。CelebA的多样性很好，有约10万张带微笑属性的数据。  http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html  Surveillance Cameras Face Database  是人脸静态图像的数据库。图像是在不受控制的室内环境中使用五种不同品质的视频监控摄像机拍摄的。数据库包含130个主题的4160静态图像（在可见和红外光谱中）。  http://www.scface.org/  Japanese Female Facial Expression （JAFFE）Database  该数据库包含由10名日本女性模特组成的7幅面部表情（6个基本面部表情+1个中性）的213幅图像。每个图像被60个日语科目评为6个情感形容词。  http://www.kasrl.org/jaffe.html  表 5 常用的手势分析数据库 数据库  简介  地址  LISA_HD_Static  这个数据库主要用来检测车内司机的行为，对车内司机的手、手机、方向盘进行了标注，用来分析司机是否在驾驶途中使用手机。  http://cvrr.ucsd.edu/vivachallenge/index.php/hands/hand-detection/  11k Hands  11k Hands数据集涵盖了来自190个18-75岁之间的实验对象的手部图像（1600 x 1200像素）。每个实验对象都要张开或是握紧左右手的手指，然后，在统一的白色背景，且手距离相机位置大致相同的情况下，分别从每只手的背侧和手掌侧进行拍摄。 元数据的记录包括：（1）实验对象ID、（2）性别、（3）年龄、（4）肤色、以及（5）有关所捕捉到的手的一组信息，即右手或左手、手的部位（手背或手掌）以及逻辑指标，即指示手部图像是否包含配饰、指甲油或不符合规则处。所提出的数据集具有大量附加更为详尽的元数据的手部图像。 该数据集是免费的，可用于合理的学术研究直接使用。  https://arxiv.org/abs/1711.04322  Visual Geometry group Hand Dataset  数据库是由OXFORD Google发布的，并发布了相关的代码。数据集的目的是对静止图片的手进行检测定位。  http://www.robots.ox.ac.uk/~vgg/data/hands/index.html  Thomas Moeslundts  这个数据集发布于1996年，实验者在深色背景前做不同手语字母。  http://www-prima.inrialpes.fr/FGnet/data/12-MoeslundGesture/database.html  2.2 新兴的研究 2.2.1 网络海量数据的[$情感计算#ai_tec*] 随着时代的发展，网络赋予[$情感计算#ai_tec*]新的、更大的数据平台，打开了[$情感计算#ai_tec*]的新局面。网络系统由于沟通了人类的现实世界和虚拟世界，可以持续不断地对数量庞大的样本进行情感跟踪，每天这些映射到网络上的情绪不计其数，利用好这些数据反过来就可以验证心理学结论，甚至反哺心理学。由于大数据的分布范围极其广泛，样本数量非常庞大，采用单一的大数据处理方法往往得不到有效的情感要素，统计效果较差。但是，如果将大数据和心理学结合起来，局面就会大不一样：心理学中，不同情感可以采用维度标定，如冷暖或软硬，同时各种心理效应影响人类对事物的情感判断，如连觉效应、视觉显著性、视觉平衡等，在大数据中引入心理学效应和维度，对有效数据进行心理学情感标准划分，使得数据具有情感维度，这样就会让计算机模拟人类情感的准确性大大提升。网络海量数据的情感主要有以下几个社会属性： . 情感随群体的变化：在社交网络，如论坛、网络社区等群体聚集的平台上流露出群体的情感，通过这些情感展现可以达到影响其他个人的行为。 . 情感随图片的变化：在社交媒体出现大量的图片，这些图片的颜色、光度、图片内容等各不相同。图片的特征直接影响到了观看者的情感。 . 情感随朋友的变化：在社交平台上，朋友发表的微博、微信状态等容易展现个人的情感。朋友间的关系比陌生人间的关系更加深入，所以朋友的情感更容易引起情感变化，在海量数据中，个人情感容易优先受朋友情感的影响。 . 情感随社会角色的变化：在社交网络中，个人在不同的群体所处的角色也不一样，个人情感流露时也会跟着所处的角色不一样而展现不同的情感。 . 情感随时间的演变：人的情绪是变化无常的，所处的环境不一样，则表现出来的情感也将不一样。即使是同一件事，不同的情景下展现的情感也会不一样。另外，事件的发展是个动态的过程，随着事件的演变，人的情感也会跟着变化。 以全球Top5初创企业之一[@Affectiva#Org*]为例，将实验环境从受控实验室转移到现实世界，搜集真实世界的数据，用来训练算法在有限条件下识别面部表情，这已成为[$Affectiva#Org*]的一个竞争优势。该公司建立了一个独特的数据库，包含在75个国家拍摄的超过500万条视频，也就是20亿个表达真实情绪的面部框架。海量数据帮助[$Affectiva#Org*]利用最新的[@机器学习#ai_tec*]方法提高准确性。鉴于数据收集的广度，[$Affectiva#Org*]可以量化文化、性别、年龄甚至环境（例如人们在家中放松还是在开车）对情绪表达方式的影响。 2.2.2 [@多模态计算#ai_tec*] 虽然人脸、姿态和语音等均能独立地表示一定的情感，但人的相互交流却总是通过信息的综合表现来进行。所以，只有实现多通道的人机界面，才是人与计算机最为自然的交互方式，它集自然语言、语音、手语、人脸、唇读、头势、体势等多种交流通道为一体，并对这些通道信息进行编码、压缩、集成和融合，集中处理图像、音频、视频、文本等多媒体信息。[$多模态计算#ai_tec*]是目前[$情感计算#ai_tec*]发展的主流方向。每个模块所传达的人类情感的信息量大小和维度不同。在人机交互中，不同的维度还存在缺失和不完善的问题。因此，[@人机交互#ai_tec*]中[@情感分析#ai_tec*]应尽可能从多个维度入手，将单一不完善的情感通道补上，最后通过多结果拟合来判断情感倾向。 在[@多模态情感计算#ai_tec*]研究中，一个很重要的分支就是[@情感机器人#ai_tec*]和[@情感虚拟人的#ai_tec*]研究。[@美国#Gep*]麻省理工学院、[@日本#Gep*]东京科技大学、[$美国#Gep*]卡内基·梅隆大学均在此领域做出了较好的演示系统。目前中科院自动化所模式识别国家重点实验室已将情感处理融入到了他们已有的语音和人脸的多模态交互平台中，使其结合情感语音合成、人脸建模、视位模型等一系列前沿技术，构筑了栩栩如生的情感虚拟头像，并积极转向嵌入式平台和游戏平台等实际应用。 目前，情感识别和理解的方法上运用了[@模式识别#ai_tec*]、[@人工智能#ai_tec*]、语音和图像技术的大量研究成果。例如：在情感语音声学分析的基础上，运用[@线性统计方法#ai_tec*]和[@神经网络模型#ai_tec*]，实现了基于语音的情感识别原型；通过对面部运动区域进行编码，采用[@HMM#ai_tec*]等不同模型，建立了面部情感特征的识别方法；通过对人姿态和运动的分析，探索肢体运动的情感类别等等。不过，受到情感信息捕获技术的影响，以及缺乏大规模的情感数据资源，有关多特征融合的[$情感理解模型#ai_tec*]研究还有待深入。随着未来的技术进展，还将提出更有效的[$机器学习#ai_tec*]机制。 2.3 [$情感计算#ai_tec*]国际会议 本节对国际会议ACM MM、ACM ICMI中涉及[$情感计算#ai_tec*]的研讨会及其代表性论文进行简单介绍。 . ACM MM-- AVEC ACM MM（ACM International Conference on Multimedia）是ACM多媒体领域的顶级会议，也是中国计算机学会推荐的A类国际学术会议。自1993年首次召开以来，ACM MM每年召开一次。ACM MM的热门方向有大规模图像视频分析、社会媒体研究、[@多模态人机交互#ai_tec*]、[@计算视觉#ai_tec*]、[@计算图像#ai_tec*]等。ACM MM包含的workshop有SUMAC、MMSports、MAHCI、AVEC、HealthMedia等。其中，AVEC（International Audio/Visual Emotion Challenge and Workshop）关注的领域是多媒体信息处理和使用[$机器学习#ai_tec*]方法来进行[$情感计算#ai_tec*]。AVEC挑战的目标是提供一个多模态的数据集来汇集多个领域的专家来尝试比较各种方法在[$情感计算#ai_tec*]上面的效果，以下是AVEC近年代表性论文。 AVEC 2018 “Bipolar Disorder and Cross-cultural Affect” AVEC 2018的主题为“Bipolar Disorder and Cross-cultural Affect”，这一挑战赛研究将多媒体处理和[$机器学习#ai_tec*]方法用于自动音频、视觉和视听健康和情感感知上的应用比较。挑战的目标是为多模态信息处理提供一个通用的基准测试集，并将音频、视觉和视听情感识别社区聚集在一起，比较自动健康和[$情感分析#ai_tec*]方法的相对优点。挑战的另一动机是推进健康和情感识别系统，以便能够处理大量未分段、非原型和非预选数据中的完全自然行为，因为人机／人机器人通信接口必须面对现实世界，而现实世界中多媒体和数据的类型大多未分段、非原型和非预选。此届挑战任务包括：Bipolar Disorder Sub-Challenge、Cross-cultural Emotion Sub-Challenge和Gold-standard Emotion Sub-Challenge。 表 6 AVEC 2018 Sub-Challenge Winners Sub-Challenge  Paper  Authors  Bipolar Disorder  Bipolar Disorder Recognition with Histogram Features of Arousal and Body Gestures.  Le Yang, Yan Li, Haifeng Chen, Dongmei Jiang, Meshia Cédric Oveneke, Hichem Sahli  Bipolar Disorder Recognition via Multi-scale Discriminative Audio Temporal Representation.  hengyin Du, Weixin Li, Di Huang, Yunhong Wang  Multi-modality Hierarchical Recall based on GBDTs for Bipolar Disorder Classification.  Xiaofen Xing, Bolun Cai, Yinhu Zhao, Shuzhen Li, Zhiwei He, Weiquan Fan  Automated Screening for Bipolar Disorder from Audio/Visual Modalities.  Zafi Sherhan Syed, Kirill A. Sidorov, A. David Marshall  Cross-cutural Emotion  Speech-based Continuous Emotion Prediction by Learning Perception Responses related to Salient Events: A Study based on Vocal Affect Bursts and Cross-Cultural Affect in AVEC 2018.  Kalani Wataraka Gamage, Ting Dang, Vidhyasaharan Sethu, Julien Epps, Eliathamby Ambikairajah  Multimodal Continuous Emotion Recognition with Data Augmentation Using Recurrent Neural Networks.  Jian Huang, Ya Li, Jianhua Tao, Zheng Lian, Mingyue Niu, Minghao Yang  Multi-modal Multi-cultural Dimensional Continues Emotion Recognition in Dyadic Interactions  Jinming Zhao, Ruichen Li, Shizhe Chen, Qin Jin  Gold-standard Emotion  Towards a Better Gold Standard: Denoising and Modelling Continuous Emotion Annotations Based on Feature Agglomeration and Outlier Regularisation.  Chen Wang, Phil Lopes, Thierry Pun, Guillaume Chanel:  Fusing Annotations with Majority Vote Triplet Embeddings.  Brandon M. Booth, Karel Mundnich, Shrikanth Narayanan:    AVEC 2017 “Real-life Depression and Affect” AVEC 2017的主题为“Real-life Depression and Affect”，其作为第七届挑战，旨在用于情绪和抑郁的自动视听分析的多媒体处理和[$机器学习#ai_tec*]方法比较。挑战的目标是在严格的可比条件下，比较用于视听情感识别和评估抑郁症的严重程度的方法的相对优点，以及在何种程度上方法的融合是可行并有益的。比赛的动机之一是推动情绪识别和抑郁严重程度估计的多媒体检索，以实现人与人、或人与人现实交互过程中能够可靠地感知。AVEC 2017有助于通过挑战来提高情绪和抑郁水平的检测标准，并基于现实生活条件下捕获的视听数据进行检测，它能够弥合关于情绪和抑郁认知的优秀研究和低可比性结果之间的差距。此届挑战任务包括：Affect Sub-Challenge, Depression Sub-Challenge。 表 7 AVEC 2017 Sub-Challenge Winners Sub-Challenge  Paper  Authors  Affect  Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network.  Jian Huang, Ya Li, Jianhua Tao, Zheng Lian, Zhengqi Wen, Minghao Yang, Jiangyan Yi  Multimodal Multi-task Learning for Dimensional and Continuous Emotion Recognition  Shizhe Chen, Qin Jin, Jinming Zhao, Shuai Wang  Investigating Word Affect Features and Fusion of Probabilistic Predictions Incorporating Uncertainty in AVEC 2017.  Ting Dang, Brian Stasak, Zhaocheng Huang, Sadari Jayawardena, Mia Atcheson, Munawar Hayat, Phu Ngoc Le, Vidhyasaharan Sethu, Roland Goecke, Julien Epps  Depression  Depression Severity Prediction Based on Biomarkers of Psychomotor Retardation.  Zafi Sherhan Syed, Kirill A. Sidorov, A. David Marshall  Hybrid Depression Classification and Estimation from Audio Video and Text Information.  Le Yang, Hichem Sahli, Xiaohan Xia, Ercheng Pei, Meshia Cédric Oveneke, Dongmei Jiang  Multimodal Measurement of Depression Using Deep Learning Models.  Le Yang, Dongmei Jiang, Xiaohan Xia, Ercheng Pei, Meshia Cédric Oveneke, Hichem Sahli  A Random Forest Regression Method With Selected-Text Feature For Depression Assessment.  Bo Sun, Yinghui Zhang, Jun He, Lejun Yu, Qihua Xu, Dongliang Li, Zhaoying Wang  Topic Modeling Based Multi-modal Depression Detection  Yuan Gong, Christian Poellabauer  AVEC 2016 “Depression, Mood and Emotion” AVEC 2016的主题为“Depression, Mood and Emotion”，其作为第六届挑战，旨在比较用于自动音频、视觉和生理抑郁和情绪分析的多媒体处理和[$机器学习#ai_tec*]方法的比较。挑战的目标是为多模态信息处