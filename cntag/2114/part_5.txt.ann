理提供一个共同的基准测试集，并将抑郁和情感识别社区以及音频、视频和生理处理社区聚集在一起，以比较各自相对优点。此届挑战任务包括：Emotion Detection, Depression Recognition。 表 8 AVEC 2016 Sub-Challenge Winners Sub-Challenge  Paper  Authors  Depression recognition  Detecting Depression using Vocal, Facial and Semantic Communication Cues.  James R. Williamson, Elizabeth Godoy, Miriam Cha, Adrianne Schwarzentruber, Pooya Khorrami, Youngjune Gwon, Hsiang-Tsung Kung, Charlie K. Dagli, Thomas F. Quatieri  Staircase Regression in OA RVM, Data Selection and Gender Dependency in AVEC 2016.  Zhaocheng Huang, Brian Stasak, Ting Dang, Kalani Wataraka Gamage, Phu Ngoc Le, Vidhyasaharan Sethu, Julien Epps  Depression Assessment by Fusing High and Low Level Features from Audio, Video, and Text.  Anastasia Pampouchidou, Olympia Simantiraki, Amir Fazlollahi, Matthew Pediaditis, Dimitris Manousos, Alexandros Roniotis, Giorgos A. Giannakakis, Fabrice Mériaudeau, Panagiotis G. Simos, Kostas Marias, Fan Yang, Manolis Tsiknakis  DepAudioNet: An Efficient Deep Model for Audio based Depression Classification.  Xingchen Ma, Hongyu Yang, Qiang Chen, Di Huang, Yunhong Wang  Multimodal and Multiresolution Depression Detection from Speech and Facial Landmark Features.  Md. Nasir, Arindam Jati, Prashanth Gurunath Shivakumar, Sandeep Nallan Chakravarthula, Panayiotis G. Georgiou  High-Level Geometry-based Features of Video Modality for Emotion Prediction. 51-58  Rapha.l Weber, Vincent Barrielle, Catherine Soladié, Renaud Séguier  Emotion Detection  Online Affect Tracking with Multimodal Kalman Filters.  Krishna Somandepalli, Rahul Gupta, Md. Nasir, Brandon M. Booth, Sungbok Lee, Shrikanth S. Narayanan  Continuous Multimodal Human Affect Estimation using Echo State Networks.  Mohammadreza Amirian, Markus K.chele, Patrick Thiam, Viktor Kessler, Friedhelm Schwenker  Multimodal Emotion Recognition for AVEC 2016 Challenge.  Filip Povolny, Pavel Matejka, Michal Hradis, Anna Popková, Lubomir Otrusina, Pavel Smrz, Ian Wood, Cécile Robin, Lori Lamel  Exploring Multimodal Visual Features for Continuous Affect Recognition.  Bo Sun, Siming Cao, Liandong Li, Jun He, Lejun Yu  . ACM ICMI--EmotiW ACM ICMI（ACM International Conference on Multimodal Interaction）是多模式人、人和[@人机交互#ai_tec*]，界面和系统开发的多学科研究的首要国际论坛。会议重点关注理论和实证基础，组件技术以及定义多模态交互分析，界面设计和系统开发领域的组合多模式处理技术。ICMI包含一系列workshop与challenge。Challenge主要包含三个，其中，challenge中参与人数最多、举办时间最长的是Emotion Recognition in the Wild Challenge (EmotiW)。EmotiW挑战重点关注无约束条件下的情感感知和基于嵌入式音频视频的情感分类和基于图像的组级面部表情识别，以模仿现实世界的情况，以下是EmotiW近年代表性论文。 表 9 EmotiW 2017 Sub-Challenge Winners Sub-Challenge  Paper  Authors  Audio-Video Sub-challenge  Learning Supervised Scoring Ensemble for Emotion Recognition in the Wild  Ping Hu, Dongqi Cai, Shandong Wang, Anbang Yao and Yurong Chen  Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video  Boris Knyazev, Roman Shvetsov, Natalia Efremova, Artem Kuharenko  Temporal Multimodal Fusion for Video Emotion Classification in the Wild  Valentin Vielzeuf, Stéphane Pateux and Frederic Jurie  Group-based Sub-challenge  Group Emotion Recognition with Individual Facial Emotion CNNs and Global Image Based CNNs  Lianzhi Tan, Kaipeng Zhang, Kai Wang, Xiaoxing Zeng, Xiaojiang Peng and Yu Qiao  Group-Level Emotion Recognition using Deep Models on Image Scene, Faces, and Skeletons  Xin Guo, Luisa Polania and Kenneth Barner  A New Deep-Learning Framework for Group Emotion Recognition  Qinglan Wei, Yijia Zhao, Qihua Xu, Liandong Li, Jun He, Lejun Yu and Bo Sun    表 10 EmotiW 2016 Sub-Challenge Winners Sub-Challenge  Paper  Authors  Video based emotion recognition  Video-based Emotion Recognition Using CNN-RNN and C3D Hybrid Networks  Fan, Yin, Xiangju Lu, Dian Li, and Yuanliu Liu  HoloNet: towards robust emotion recognition in the wild  Yao, Anbang, Dongqi Cai, Ping Hu  Emotion Recognition in the Wild from Videos using Images  Bargal, Sarah Adel, Emad Barsoum, Cristian Canton Ferrer, and Cha Zhang  Multi-cue Fusion for Emotion Recognition in the Wild  Yan, Jingwei, Wenming Zheng, Zhen Cui, Chuangao Tang, Tong Zhang, and Yuan Zong  Group based emotion recognition  Happiness Level Prediction with Sequential Inputs via Multiple Regressions  Li, Jianshu, Sujoy Roy, Jiashi Feng, and Terence Sim  Group happiness assessment using geometric features and dataset balancing  Vonikakis, Vassilios, Yasin Yazici, Viet Dung Nguyen, and Stefan Winkler  LSTM for Dynamic Emotion and Group Emotion Recognition in the Wild  Sun, Bo, Qinglan Wei, Liandong Li, Qihua Xu, Jun He, and Lejun Yu  2.4 情感计算获奖论文 本节对近年ACM MM中涉及情感计算的获奖论文进行简单介绍，欢迎读者交流补充。 . 2018-ACM MM Best Demo 篇名：AniDance: Real-Time Dance Motion Synthesize to the Song. 作者：Taoran Tang, Hanyang Mao, Jia Jia 单位：Tsinghua University 概述：本篇论文介绍了一个名为[@AniDance#ai_product*]的系统，它可以实时地将舞蹈动作与旋律相结合。当用户在手机中播放一首歌或者在[$AniDance#ai_product*]中播放一首歌时，歌曲旋律将驱动3D空间角色跳舞，创造出一个生动的舞蹈动画。在实践中，研究人员通过捕捉真实的舞蹈表演，来构成3D空间舞蹈运动数据集，使用[@LSTM自动编码器#ai_tec*]来识别音乐和舞蹈之间的关系。基于这些技术，用户可以提升他们的学习能力以及对舞蹈和音乐的兴趣。 下载地址：https://hcsi.cs.tsinghua.edu.cn/Paper/Paper18/MM18-TANGTAORANDEMO.pdf   . 2016-ACMMM Best Paper 篇名：Multi-modal Multi-view Topic-opinion Mining for Social Event Analysis 作者：钱胜胜、张天柱、徐常胜 单位：中国科学院自动化研究所 概述：本篇论文针对社会事件数据中呈现的多视角多模态等属性，提出一种多模态多视角的主题观点挖掘方法。该方法利用不同新闻媒体对同一个社会热点事件观点的差异性，研究不同视角对应的观点。通过本方法，既能够细致地分析不同媒体对社会热点事件的观点，又能够对社会热点事件进行全方位展示，便于探究热点事件的全局态势与舆情走势。本篇论文从多个社交平台上检测社会热点事件，并将其全面地展现出来，不仅切合政府和民众的需求，也是国际学术的研究热点。获奖论文敏锐地利用不同社交平台的在传播速度和传播内容上的特点，提出了一个有效的跨平台跨模态的社会热点事件监测方法。选题的新颖性、解决思路的创新性、以及技术的有效性，均受到评审专家的一致认可。 下载地址：http://www.millenniumsoftsol.com/courses/IEEETitles/Java/Multi-Modal-Event.pdf . 2014- ACM MM Best Student Paper Award 篇名：Say Cheese vs. Smile: Reducing Speech-Related Variability for Facial Emotion Recognition 作者：Yelin Kim and Emily Mower Provost. 单位：Electrical Engineering: Systems, University of Michigan 概述：作者认为，[@情感识别系统#ai_tec*]的应用非常有意义，比如社会和情感[$人机交互#ai_tec*]系统及与健康相关的系统可以帮助个体更好地记录他们的情感世界。但情感的表达是非常复杂的，可以通过面部表情、语音语调以及肢体语言的变化来体现。[$情感识别系统#ai_tec*]必须能够“解码”这些外在的变化来了解其代表的真实情感。但是，外在表情和行为的变化不仅仅受到情绪的影响，比如面部表情的变化有时是由于语言发音引起的。有效的[$情感识别系统#ai_tec*]必须能够辨别人的表情及行为变化是由于语言发音导致的，还是由于情感变化导致的。在本文中，作者描述了一种面部表情分析模型，目的是减少语言发音对于面部表情的影响，更加准确地识别表情变化背后的情感变化，提高面部表情的情感识别率、 下载地址：https://cn.aminer.cn/archive/say-cheese-vs-smile-reducing-speech-related-variability-for-facial-emotion-recognition/555045d945ce0a409eb5a118    . 2012- ACM MM Grand Challenge 1st Prize 篇名：The Acousticvisual Emotion Guassians Model for Automatic Generation of Music Video 作者：Ju-Chiang Wang, Yi-Hsuan Yang, I-Hong Jhuo, Yen-Yu Lin, Hsin-Min Wang: 单位：Research Center for Information Technology Innovation Academia Sinica 概述：音乐研究中最具挑战性的工作是建立能够比较音乐情感内涵，并能根据情感组织音乐的计算模型。本文中，作者提出了一种新型[@声学情感高斯模型#ai_tec*]（[@AEG#ai_tec*]）来界定音乐情感感知的过程。[$AEG#ai_tec*]作为一种生成模型，对学习过程做出了简单直接的解释。为了架起声学特征空间与音乐情感空间的桥梁，作者从数据中提取了一系列潜在特征集，并将其引入到模型中，执行两个空间端到端语义映射。[$AEG#ai_tec*]模型适用于音乐情感自动标注与基于情感的音乐检索。为了深入了解[$AEG#ai_tec*]模型，文中提供了模型学习过程的说明。为了表明[$AEG#ai_tec*]模型具有更高的精度，作者对两种音乐语料库MER60和Muturk进行了综合性能研究。结果显示，[$AEG#ai_tec*]模型在音乐情感注释方面胜过以前的方法。另外，文章首次提出了基于音乐情感检索的定量评估。 下载地址：https://cn.aminer.cn/archive/the-acousticvisual-emotion-guassians-model-for-automatic-generation-of-music-video/53e9ad4fb7602d9703736369   内页背景4.jpg3 人才篇 为了了解情感计算专家学者的分布，我们统计了最近10年（2009年至2018年），情感计算相关的期刊和会议，并按照相关关键词进行挖掘，找到了在以上限定范围内发文量最高的1000余名情感计算学者。 研究选择的期刊和会议包括以下： 表 11 情感计算相关期刊、会议 全称  分类  简称  中文  网址  American Association for Applied Linguistics  会议  AAAL  美国人工智能协会人工智能会议  https://www.aaal.org/  International Joint Conference on Artificial Intelligence  会议  IJCAI  国际人工智能联合大会  https://www.ijcai.org/  IEEE Transactions on Multimedia  期刊  TMM  电气和电子工程师协会多媒体汇刊  https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046  IEEE Transactions on Automatic Control  期刊  TAC  电气和电子工程师协会自动控制汇刊  https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9  IEEE Transactions on Knowledge and Data Engineering  期刊  TKDE  电气和电子工程师协会知识与数据工程汇刊  https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?reload=true&punumber=69  IEEE Transactions on Affective Computing  期刊  TAC  情感计算汇刊  https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165369  International Conference on Acoustics, Speech and Signal Processing  会议  ICASSP  国际声学、语音和信号处理会议  http://www.ieee-icassp2017.org/  Conference on Knowledge Discovery and Data Mining  会议  KDD  知识发现与数据挖掘会议  https://www.kdd.org/  IEEE International Conference on Multimedia and Expo  会议  ICME  电气和电子工程师协会国际多媒体与博览会会议  http://www.icme2019.org/  ACM International Conference on Multimedia  会议  ACMMM  多媒体国际会议  https://www.acmmm.org/2019/  筛选的关键词包括：expression、emotional、emotion、emotive、affect、affective、sentiment、sentimental、mental、health、stress、depression、aesthetics等。 3.1 学者情况概览 3.1.1 全球学者概况 . 学者地图 学者分布地图对于进行学者调查、分析各地区竞争力现况尤为重要，图 3为情感计算领域全球顶尖学者分布状况。其中，颜色越趋近于红色，表示学者越集中；颜色越趋近于绿色，表示学者越稀少。从地区角度来看，北美洲、欧洲是情感计算领域学者分布最为集中的地区，亚洲东部地区次之，南美洲和非洲学者极为匮乏。从国家角度来看，情感计算领域的人才在美国最多，中国次之，意大利、法国等洲国家也有较多的学者数量，整体上讲其它国家与[@美国#Gep*]的差距较大，学者数量排名前十的国家如图 4所示  图 3 情感计算全球专家分布  3131374040415152120375050100150200250300350400澳大利亚英国印度加拿大西班牙德国法国意大利中国[$美国#Gep*]图 4 [@情感计算#ai_tec*]专家国家数量分布 . 性别比例 在性别比例方面，[$情感计算#ai_tec*]领域中男性学者占比92.7%，女性学者占比7.3%，男性学者占比远高于女性学者。 . 学者h-index分布 [$情感计算#ai_tec*]领域学者的h-index分布如图 5所示，分布情况整体呈阶梯状，大部分学者的h-index分布在中低区域，其中h-index在<10的区间人数最多，有524人，占比43.4%，50-60区间人数最少，有46人，占比3.8%。  5242611741125134490100200300400500600<1010-2020-3030-4040-5050-60>60图 5 [$情感计算#ai_tec*]领域学者h-index分布  . 人才迁徙 我们对[$情感计算#ai_tec*]领域TOP学者的迁徙路径做了分析。由图 6可以看出，各国[$情感计算#ai_tec*]TOP学者的流失和引进是相对比较均衡的，其中[$美国#Gep*]是[$情感计算#ai_tec*]领域人才流动大国，人才输入和输出幅度领先于其他国家，且从数据来看人才流出大于人才流入。英国、加拿大和印度等国人才迁徙流量小于[$美国#Gep*]；[@中国#Gep*]人才流入略高于人才流出。人才的频繁流入流出，使得该领域的学术交流活动增加，带动了人才质量提升的同时，也促进了领域理论及技术的更新迭代，逐渐形成一种良性循环的过程。  图 6 [$情感计算#ai_tec*]专家迁徙图 . 中外合作 [$中国#Gep*]与其他国家在[$情感计算#ai_tec*]领域的合作情况可以根据AMiner数据平台分析得到，通过统计论文中作者的单位信息，将作者映射到各个国家中，进而统计[$中国#Gep*]与各国之间合作论文的数量，并按照合作论文发表数量从高到低进行了排序，如表 12所示。 表 12 [$情感计算#ai_tec*]领域[$中国#Gep*]与各国合作论文情况 合作国家  论文数  引用数  平均引用数  学者数  [$中国#Gep*]--[$美国#Gep*]  178  8603  48  468  [$中国#Gep*]--英国  28  1437  51  66  [$中国#Gep*]--澳大利亚  22  579  26  55  [$中国#Gep*]--新加坡  18  786  44  48  [$中国#Gep*]--加拿大  14  440  31  38  [$中国#Gep*]--日本  12  359  30  27  [$中国#Gep*]--瑞士  8  897  112  10  [$中国#Gep*]--德国  8  257  32  24  [$中国#Gep*]--瑞典  6  328  55  10  [$中国#Gep*]--法国  4  296  74  8  从上表数据可以看出，中美合作的论文数、引用数、学者数遥遥领先，表明中美间在[$情感计算#ai_tec*]领域合作之密切；从地域角度看，[$中国#Gep*]与欧洲的合作非常广泛，前10名合作关系里中欧合作共占5席；[$中国#Gep*]与瑞士合作的论文数虽然不是最多，但是拥有最高的平均引用数说明在合作质量上中瑞合作达到了较高的水平。 3.1.2 国内学者概况 . 学者地图 AMiner选取[$情感计算#ai_tec*]领域国内专家学者绘制了学者国内分布地图，如图 7所示。通过下图我们可以发现，京津地区在[$情感计算#ai_tec*]领域的人才数量最多，东部及南部沿海地区的也有较多的人才分布，相比之下，内陆地区信[$情感计算#ai_tec*]领域人才较为匮乏，这也从一定程度上说明了[$情感计算#ai_tec*]领域的发展与该地区的地理位置和经济水平都是息息相关的。同时，通过观察[$中国#Gep*]周边国家的学者数量情况，特别是与[@日本#Gep*]、东南亚等亚洲国家相比，[$中国#Gep*]在[$情感计算#ai_tec*]领域顶尖学者数量方面具有较为明显的优势。图 8是我国[$情感计算#ai_tec*]领域顶尖学者最多的10个省份。  图 7 [$情感计算#ai_tec*]国内学者分布  4567771010123205101520253035香港特别行政区广东省江苏省上海市浙江省湖北省山东省台湾省黑龙江省北京市图 8 [$情感计算#ai_tec*]学者分布国内省份TOP10 3.2 典型学者 我们在选取的期刊会议中，对所涉学者及其论文关键信息进行抽取，下面将对国内及国外人才部分代表性进行介绍。本文中“国外学者”指在国外科研单位供职的学者。 3.2.1 国外代表性学者 . Rosalind W. Picard   Rosalind W. Picard，[$美国#Gep*]麻省理工学院媒体实验室[$情感计算#ai_tec*]研究组创始人兼主任，[$美国#Gep*]电气电子工程师学会（IEEE）院士，创业公司Affectiva与Empatica的联合创办人。她于1995年正式提出“[$情感计算#ai_tec*]（Affective Computing）”的概念，为人工智能奠定了理论基础。她指出“[$情感计算#ai_tec*]就是针对人类的外在表现，能够进行测量和分析并能对情感施加影响的计算”，开辟了计算机科学的新领域，其思想是使计算机拥有情感，能够像人一样识别和表达情感，从而使[$人机交互#ai_tec*]更自然，她所在的实验室通过非常动态技术的推进来追踪不同的情感，并理解情感的发生。 . Jeffrey F. Cohn   Jeffr