网络的摄影、图像处理应用能够为用户提供更加完美的体验，图17 显示了 Mate10 的成像效果对比图。 图 17 华为 Mate10 成像效果对比图 2017 年 9 月中旬，苹果发布以iPhone X 为代表的手机及它们内置的 A11 Bionic 芯片。31A11 Bionic 中自主研发的双核架构 Neural Engine（神经网络处理引擎），它每秒处理相应神经网络计算需求的次数可达6000 亿次。这个 Neural Engine 的出现，让A11 Bionic 成为一块真正的AI 芯片。A11 Bionic 大大提升了iPhone X 在拍照方面的使用体验，并提供了一些富有创意的新用法。如更具革命性的 FaceID，它能够将传感器数据进行实时3D建模，并利用机器学习识别用户容貌改变，在此过程中的大量计算需求，都需要借助 A11Bionic 和Neural Engine 来满足，如图18 所示。除此之外，A11 Bionic 内置了苹果自主设计的第一款GPU。这款 GPU 是为3D 游戏和 Metal 2（苹果在WWDC 2017上推出的新一代图像渲染技术框架）专门设计的，并且能够与机器学习技术和苹果随 iOS 11 推出的Core ML（核心机器学习）框架相配合。图 18 苹果的Face ID 谷歌，高通同样在随后发布的产品中植入AI 芯片，或许这将成为业界的一个新趋势，即使因为植入 AI 芯片能为用户带来真正美好体验，还需要等到有足够多的基于深度学习的APP 出现才可实现。 （2）ADAS（高级辅助驾驶系统） ADAS是最吸引大众眼球的人工智能应用之一，它需要处理海量的由激光雷达、毫米波雷达、摄像头等传感器采集的实时数据。ADAS 的中枢大脑——ADAS 芯片市场的主要厂商包括被英特尔收购的 Mobileye、2017 年被高通以 470 亿美元惊人价格收购的 NXP，以及汽车电子的领军企业英飞凌。随着英伟达推出自家基于 GPU的ADAS解决方案 Drive PX2，英伟达也加入到战团之中。 相对于传统的车辆控制方法，智能控制方法主要体现在对控制对象模型的运用和综合32信息学习运用上，包括神经网络控制和深度学习方法等，得益于 AI 芯片的飞速发展，这些算法已逐步在车辆控制中得到应用。 （3）CV（计算机视觉（Computer Vision））设备 需要使用计算机视觉技术的设备，如智能摄像头、无人机、行车记录仪、人脸识别迎宾机器人以及智能手写板等设备，往往都具有本地端推断的需要，如果仅能在联网下工作，无疑将带来糟糕的体验。而计算机视觉技术目前看来将会成为人工智能应用的沃土之一，计算机视觉芯片将拥有广阔的市场前景。 计算机视觉领域全球领先的芯片提供商 Movidius，目前已被英特尔收购，大疆无人机、海康威视和大华股份的智能监控摄像头均使用了 Movidius 的 Myriad 系列芯片。 目前国内做计算机视觉技术的公司以初创公司为主，如商汤科技、阿里系旷视、腾讯优图，以及云从、依图等公司。在这些公司中，未来有可能随着其自身计算机视觉技术的积累渐深，部分公司将会自然而然转入 CV 芯片的研发中，正如Movidius 走的也是从计算机视觉技术到芯片研发的路径。 （4）VR 设备 VR 设备芯片的代表为 HPU 芯片，是微软为自身 VR设备Hololens 研发定制的。这颗由台积电代工的芯片能同时处理来自5 个摄像头、1个深度传感器以及运动传感器的数据，并具备计算机视觉的矩阵运算和CNN运算的加速功能。这使得 VR 设备可重建高质量的人像3D 影像，并实时传送到任何地方。 （5）语音交互设备 语音交互设备芯片方面，国内有启英泰伦以及云知声两家公司，其提供的芯片方案均内置了为语音识别而优化的深度神经网络加速方案，实现设备的语音离线识别。稳定的识别能力为语音技术的落地提供了可能；与此同时，语音交互的核心环节也取得重大突破。语音识别环节突破了单点能力，从远场识别，到语音分析和语义理解有了重大突破，呈现出一种整体的交互方案。 语音交互正在悄悄改变人们的家居生活习惯，如居于客厅核心位置的智能电视，越来越多的消费者习惯在沙发上使用语音换台，语音作为智能家居入口将有广阔的想象空间。 （6）机器人 无论是家居机器人还是商用服务机器人均需要专用软件+芯片的人工智能解决方案，这方面典型公司有由前百度深度学习实验室负责人余凯创办的地平线机器人，当然地平线机器人除此之外，还提供 ADAS、智能家居等其他嵌入式人工智能解决方案。 在移动端推断领域，呈现给我们的是一个缤纷的生态。因为无论是ADAS还是各类CV、33VR 等设备领域，人工智能应用仍远未成熟，各人工智能技术服务商在深耕各自领域的同时，逐渐由人工智能软件演进到软件+芯片解决方案是自然而然的路径，因此形成了丰富的芯片产品方案。 346 趋势篇 目前主流AI 芯片的核心主要是利用MAC（Multiplier and Accumulation，乘加计算）加速阵列来实现对CNN（卷积神经网络）中最主要的卷积运算的加速。这一代 AI 芯片主要有如下3 个方面的问题。 （1）深度学习计算所需数据量巨大，造成内存带宽成为整个系统的瓶颈，即所谓“memory wall”问题。 （2）与第一个问题相关，内存大量访问和MAC 阵列的大量运算，造成AI 芯片整体功耗的增加。 （3）深度学习对算力要求很高，要提升算力，最好的方法是做硬件加速，但是同时深度学习算法的发展也是日新月异，新的算法可能在已经固化的硬件加速器上无法得到很好的支持，即性能和灵活度之间的平衡问题。 因此，我们可以预见，下一代 AI 芯片将有如下的几个发展趋势。 趋势一：更高效的大卷积解构/复用 在标准SIMD 的基础上，CNN 由于其特殊的复用机制，可以进一步减少总线上的数据通信。而复用这一概念，在超大型神经网络中就显得格外重要。如何合理地分解、映射这些超大卷积到有效的硬件上成为了一个值得研究的方向，如图19 所示。图 19 分解卷积可降低消耗 趋势二：更低的 Inference 计算/存储位宽AI 芯片最大的演进方向之一可能就是神经网络参数/计算位宽的迅速减少——从 32 位36浮点到16 位浮点/定点、8 位定点，甚至是 4 位定点。在理论计算领域，2 位甚至1 位参数位宽，都已经逐渐进入实践领域，如图 20 所示。 图 20 逐层动态定点方法 趋势三：更多样的存储器定制设计 当计算部件不再成为神经网络加速器的设计瓶颈时，如何减少存储器的访问延时将会成为下一个研究方向。通常，离计算越近的存储器速度越快，每字节的成本也越高，同时容量也越受限，因此新型的存储结构也将应运而生。 趋势四：更稀疏的大规模向量实现 图 21 五级流水线结构 神经网络虽然大，但是，实际上有很多以零为输入的情况，此时稀疏计算可以高效的减少无用能效。来自哈佛大学的团队就该问题提出了优化的五级流水线结构，如图 21 所示，在最后一级输出了触发信号。在Activation 层后对下一次计算的必要性进行预先判断，如果发现这是一个稀疏节点，则触发SKIP 信号，避免乘法运算的功耗，以达到减少无用功耗的37目的。 趋势五：计算和存储一体化 计算和存储一体化（process-in-memory）技术，其要点是通过使用新型非易失性存储（如 ReRAM）器件，在存储阵列里面加上神经网络计算功能，从而省去数据搬移操作，即实现了计算存储一体化的神经网络处理，在功耗性能方面可以获得显著提升。38 