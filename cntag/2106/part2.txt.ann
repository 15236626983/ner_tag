潮。从基础算法、底层硬件、工具框架到实际应用场景，现阶段的人工智能领域已经全面开花。 作为[@人工智能#ai_tec*]核心的底层硬件[@ AI 芯片#ai_product*]，也同样经历了多次的起伏和波折，总体看来，[@AI 芯片#ai_product*]的发展前后经历了四次大的变化，其发展历程如图 2 所示。 4 图 2[$ AI 芯片#ai_product*]发展历程 （1）[@2007#Date*] 年以前，AI芯片产业一直没有发展成为成熟的产业；同时由于当时算法、数据量等因素，这个阶段[$AI 芯片#ai_product*]并没有特别强烈的市场需求，通用的 CPU芯片即可满足应用需要。 （2）随着高清视频、VR、AR 游戏等行业的发展，GPU产品取得快速的突破；同时人们发现GPU的并行计算特性恰好适应[$人工智能#ai_tec*]算法及大数据并行计算的需求，如 GPU 比之前传统的 CPU在[@深度学习算法#ai_tec*]的运算上可以提高几十倍的效率，因此开始尝试使用GPU进行[$人工智能#ai_tec*]计算。 （3）进入 [@2010#Date*] 年后，云计算广泛推广，[$人工智能#ai_tec*]的研究人员可以通过云计算借助大量CPU 和GPU 进行混合运算，进一步推进了[$ AI 芯片#ai_product*]的深入应用，从而催生了各类[$ AI 芯片#ai_product*]的研发与应用。 （4）[$人工智能#ai_tec*]对于计算能力的要求不断快速地提升，进入 [@2015#Date*] 年后，GPU 性能功耗比不高的特点使其在工作适用场合受到多种限制，业界开始研发针对[$人工智能#ai_tec*]的专用芯片，以期通过更好的硬件和芯片架构，在计算效率、能耗比等性能上得到进一步提升。 1.3 我国[$ AI 芯片#ai_product*]发展情况 目前，我国的[$人工智能#ai_tec*]芯片行业发展尚处于起步阶段。长期以来，[@中国#Gpe*]在CPU、GPU、DSP 处理器设计上一直处于追赶地位，绝大部分芯片设计企业依靠国外的 IP 核设计芯片，在自主创新上受到了极大的限制。然而，[$人工智能#ai_tec*]的兴起，无疑为[$中国#Gpe*]在处理器领域实现5弯道超车提供了绝佳的机遇。[$人工智能#ai_tec*]领域的应用目前还处于面向行业应用阶段，生态上尚未形成垄断，国产处理器厂商与国外竞争对手在[$人工智能#ai_tec*]这一全新赛场上处在同一起跑线上，因此，基于新兴技术和应用市场，[$中国#Gpe*]在建立[$人工智能#ai_tec*]生态圈方面将大有可为。 由于我国特殊的环境和市场，国内[$ AI 芯片#ai_product*]的发展目前呈现出百花齐放、百家争鸣的态势，[$AI 芯片#ai_product*]的应用领域也遍布股票交易、金融、商品推荐、安防、[@早教机器人#ai_product*]以及[@无人驾驶#ai_tec*]等众多领域，催生了大量的[$人工智能#ai_tec*]芯片创业公司，如[@地平线#Org*]、[@深鉴科技#Org*]、[@中科寒武纪#Org*]等。 尽管如此，国内公司却并未如国外大公司一样形成市场规模，反而出现各自为政的散裂发展现状。除了新兴创业公司，国内研究机构如[@北京大学#Org*]、[@清华大学#Org*]、[@中国科学院#Org*]等在[$AI 芯片#ai_product*]领域都有深入研究；而其他公司如[@百度#Org*]和[@比特大陆#Org*]等，[@2017#Date*] 年也有一些成果发布。 可以预见，未来谁先在[$人工智能#ai_tec*]领域掌握了生态系统，谁就掌握住了这个产业的主动权。 6[@ASIC 芯片#ai_product*]非常适合[$人工智能#ai_tec*]的应用场景。首先，ASIC的性能提升非常明显。例如[@英伟达#Org*]首款专门为[@深度学习#ai_tec*]从零开始设计的芯片[@TeslaP100#ai_tec*] 数据处理速度是其 [@2014#Date*] 年推出GPU系列的 12 倍。[@谷歌#Org*]为定制的芯片TPU 将硬件性能提升至相当于当前芯片按摩尔定律发展 7 年后的水平。正如 CPU改变了当年庞大的计算机一样，[$人工智能#ai_tec*] ASIC 芯片也将大幅改变如今 AI 硬件设备的面貌。如大名鼎鼎的 [@AlphaGo#ai_product*] 使用了约170 个图形处理器（GPU）和1200 个中央处理器（CPU），这些设备需要占用一个机房，还要配备大功率的空调，以及多名专家进行系统维护。而如果全部使用专用芯片，极大可能只需要一个普通收纳盒大小的空间，，且功耗也会大幅降低。 第二，下游需求促进[$人工智能#ai_tec*]芯片专用化。从服务器，计算机到无人驾驶汽车、[@无人机#ai_product*]再到[@智能家居#ai_product*]的各类家电，至少数十倍于智能手机体量的设备需要引入感知交互能力和[$人工智能#ai_tec*]计算能力。而出于对实时性的要求以及训练数据隐私等考虑，这些应用不可能完全依赖云端，必须要有本地的软硬件基础平台支撑，这将带来海量的[$人工智能#ai_tec*]芯片需求。 目前[@人工智能专用芯片#ai_product*]的发展方向包括：主要基于FPGA的半定制、针对[$深度学习算法#ai_tec*]的全定制和[@类脑计算芯片#ai_product*]3 个方向。 在芯片需求还未形成规模、[$深度学习算法#ai_tec*]暂未稳定，[$AI 芯片#ai_product*]本身需要不断迭代改进的情况下，利用具备可重构特性的 FPGA 芯片来实现半定制的[$人工智能#ai_tec*]芯片是最佳选择之一。这类芯片中的杰出代表是国内初创公司深鉴科技，该公司设计了“深度学习处理单元”（Deep Processing Unit，[@DPU#ai_product*]）的芯片，希望以 ASIC 级别的功耗达到优于 GPU 的性能，其第一批产品就是基于 FPGA 平台开发研制出来的。这种半定制芯片虽然依托于FPGA 平台，但是抽象出了指令集与编译器，可以快速开发、快速迭代，与专用的FPGA 加速器产品相比，也具有非常明显的优势。 [$深度学习算法#ai_tec*]稳定后，AI芯片可采用ASIC 设计方法进行全定制，使性能、功耗和面积等指标面向[$深度学习算法#ai_tec*]做到最优。 2.5 [@类脑芯片#ai_product*] 类脑芯片不采用经典的冯·诺依曼架构，而是基于神经形态架构设计，以[@IBM#Org*] [@Truenorth#ai_product*] 为代表。[@IBM#Org*]研究人员将存储单元作为突触、计算单元作为神经元、传输单元作为轴突搭建了神经芯片的原型。目前，Truenorth用三星 28nm 功耗工艺技术，由54 亿个晶体管组成的芯片构成的片上网络有 4096 个神经突触核心，实时作业功耗仅为 70mW。由于神经突触要求权重可变且要有记忆功能，IBM 采用与 CMOS工艺兼容的相变非挥发存储器（PCM）的技术实验性的实现了新型突触，加快了商业化进程。 在国内，[$清华大学#Org*]类脑计算中心于 [$2015#Date*] 年11 月成功的研制了国内首款超大规模的神经形态类脑计算天机芯片。该芯片同时支持脉冲神经网络和人工神经网络（深度神经网络），13可进行大规模神经元网络的模拟。中心还开发了面向类脑芯片的工具链，降低应用的开发难度并提升效率。第二代 28nm 天机芯片也已问世，在性能功耗比上要优于Truenorth。 当前，类脑[$ AI 芯片#ai_product*]的设计目的不再仅仅局限于加速[$深度学习算法#ai_tec*]，而是在芯片基本结构甚至器件层面上改变设计，希望能够开发出新的类脑计算机体系结构，比如采用忆阻器和ReRAM 等新器件来提高存储密度。这类芯片技术尚未完全成熟，离大规模应用还有一定的差距，但是长期来看类脑芯片有可能会带来计算机体系结构的革命。 2.6 AI芯片技术特点比较 通过以上分析，我们可以总结出以下几个特点。 l CPU 通用性最强，但延迟严重，散热高，效率最低。 l GPU通用性强、速度快、效率高，特别适合用在[$深度学习#ai_tec*]训练方面，但是性能功耗比较低。 l FPGA具有低能耗、高性能以及可编程等特性，相对于CPU 与 GPU有明显的性能或者能耗优势，但对使用者要求高。 l ASIC 可以更有针对性地进行硬件层次的优化，从而获得更好的性能、功耗比。但是[$ASIC 芯片#ai_product*]的设计和制造需要大量的资金、较长的研发周期和工程周期，而且[$深度学习算法#ai_tec*]仍在快速发展，若[$深度学习算法#ai_tec*]发生大的变化，FPGA 能很快改变架构，适应最新的变化，ASIC类芯片一旦定制则难于进行修改。 当前阶段，GPU配合 CPU 仍然是[$AI 芯片#ai_product*]的主流，而后随着视觉、语音、[$深度学习#ai_tec*]的算法在FPGA 以及[$ASIC 芯片#ai_product*]上的不断优化，此两者也将逐步占有更多的市场份额，从而与GPU达成长期共存的局面。从长远看，人工智能[@类脑神经芯片#ai_product*]是发展的路径和方向。 143 产业篇 本篇将介绍目前[@人工智能芯片技术#ai_tec*]领域的国