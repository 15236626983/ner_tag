 1 概述篇 [@人工智能（ArtificialIntelligence，AI）芯片#ai_product*]的定义：从广义上讲只要能够运行[@人工智能算法#ai_tec*]的芯片都叫作[@AI 芯片#ai_product*]。但是通常意义上的[@AI 芯片#ai_product*]指的是针对[$人工智能算法#ai_tec*]做了特殊加速设计的芯片，现阶段，这些[$人工智能算法#ai_tec*]一般以[@深度学习算法#ai_tec*]为主，也可以包括其它[@机器学习算法#ai_tec*]。[@人工智能#ai_tec*]与[@深度学习#ai_tec*]的关系如图1 所示。 图 1 [@人工智能#ai_tec*]与深度学习 [$深度学习算法#ai_tec*]，通常是基于接收到的连续数值，通过学习处理，并输出连续数值的过程，实质上并不能完全模仿生物大脑的运作机制。基于这一现实，研究界还提出了 [@SNN#ai_tec*]（[@Spiking Neural Network#ai_tec*]，[@脉冲神经网络#ai_tec*]）模型。作为第三代[@神经网络模型#ai_tec*]，[$SNN#ai_tec*]更贴近生物神经网络——除了神经元和突触模型更贴近生物神经元与突触之外，[$SNN#ai_tec*]还将时域信息引入了计算模型。目前基于 [$SNN#ai_tec*] 的[$AI 芯片#ai_product*]主要以 [@IBM #Org*]的 [@TrueNorth#ai_product*]、[@Intel#ai_product*] 的 [@Loihi#ai_product*] 以及国内的[@清华大学#Org*][@天机芯#ai_product*]为代表。 1.1 [@AI芯片#ai_product*]的分类 （1）[$AI 芯片#ai_product*]按技术架构分类 GPU（Graphics Processing Unit，图形处理单元）：在传统的[@冯·诺依曼#Person*]结构中，CPU每执行一条指令都需要从存储器中读取数据，根据指令对数据进行相应的操作。从这个特点可以看出，CPU 的主要职责并不只是数据运算，还需要执行存储读取、指令分析、分支跳转等命令。[$深度学习算法#ai_tec*]通常需要进行海量的数据处理，用 CPU执行算法时，CPU将花费大量的时间在数据/指令的读取分析上，而 CPU 的频率、内存的带宽等条件又不可能无限制提高，因此限制了处理器的性能。而GPU 的控制相对简单，大部分的晶体管可以组成各类专用电路、多条流水线，使得 GPU的计算速度远高于CPU；同时GPU拥有了更加强大的浮点运算能力，可以缓解[$深度学习算法#ai_tec*]的训练难题，释放[$人工智能#ai_tec*]的潜能。 2但GPU 无法单独工作，必须由CPU 进行控制调用才能工作，而且功耗比较高。 半定制化的 FPGA：FPGA（Field Programmable Gate Array)全称“现场可编程门阵列”，其基本原理是在 FPGA 芯片内集成大量的基本门电路以及存储器，用户可以通过更新FPGA配置文件来定义这些门电路以及存储器之间的连线。 与GPU 不同，FPGA 同时拥有硬件流水线并行和数据并行处理能力，适用于以硬件流水线方式处理一条数据，且整数运算性能更高，因此常用于[$深度学习算法#ai_tec*]中的推断阶段。不过 FPGA通过硬件的配置实现软件算法，因此在实现复杂算法方面有一定的难度。将FPGA和 CPU对比可以发现两个特点，一是 FPGA 没有内存和控制所带来的存储和读取部分，速度更快，二是FPGA没有读取指令操作，所以功耗更低。劣势是价格比较高、编程复杂、整体运算能力不是很高。目前国内的 [$AI 芯片#ai_product*]公司如[@深鉴科技#Org*]就提供基于 FPGA 的解决方案。 全定制化 [@ASIC#ai_product*]：[$ASIC#ai_product*]c（Application-Specific Integrated Circuit）专用集成电路，是专用定制芯片，即为实现特定要求而定制的芯片。定制的特性有助于提高 [$ASIC#ai_product*] 的性能功耗比，缺点是电路设计需要定制，相对开发周期长，功能难以扩展。但在功耗、可靠性、集成度等方面都有优势，尤其在要求高性能、低功耗的移动应用端体现明显。[@谷歌#Org*]的[@TPU#ai_product*]、[@寒武纪#Org*]的 [@GPU#ai_product*]，[@地平线#Org*]的[@BPU#ai_product*] 都属于 [$ASIC#ai_product*] 芯片。谷歌的TPU 比 CPU 和GPU 的方案快30至80 倍，与 CPU 和 GPU相比，TPU把控制电路进行了简化，因此减少了芯片的面积，降低了功耗。 [@神经拟态芯片#ai_product*]：神经拟态计算是模拟生物神经网络的计算机制。神经拟态计算从结构层面去逼近大脑，其研究工作还可进一步分为两个层次，一是神经网络层面，与之相应的是神经拟态架构和处理器，如 [@IBM#Org*]的 [$TrueNorth#ai_product*] 芯片，这种芯片把定制化的数字处理内核当作神经元，把内存作为突触。其逻辑结构与传统[$冯·诺依曼#Person*]结构不同：它的内存、CPU 和通信部件完全集成在一起，因此信息的处理在本地进行，克服了传统计算机内存与 CPU 之间的速度瓶颈问题。同时神经元之间可以方便快捷地相互沟通，只要接收到其他神经元发过来的脉冲(动作电位)，这些神经元就会同时做动作。二是神经元与神经突触层面，与之相应的是元器件层面的创新。如[@IBM#Org*]苏黎世研究中心宣布制造出世界上首个人造纳米尺度的随机相变神经元，可实现[@高速无监督学习#ai_tec*]。 （2）[$AI 芯片#ai_product*]按功能分类 根据[$机器学习算法#ai_tec*]步骤，可分为训练（training）和推断（inference）两个环节： 训练环节通常需要通过大量的数据输入，训练出一个复杂的深度[$神经网络模型#ai_tec*]。训练过程由于涉及海量的训练数据和复杂的[@深度神经网络结构#ai_tec*]，运算量巨大，需要庞大的计算规模，对于处理器的计算能力、精度、可扩展性等性能要求很高。目前市场上通常使用英伟达的[$GPU#ai_product*] 集群来完成，[@Google#Org*]的 [$TPU#ai_product*]2.0/3.0 也支持训练环节的[@深度网络#ai_tec*]加速。 3推断环节是指利用训练好的模型，使用新的数据去“推断”出各种结论。这个环节的计算量相对训练环节少很多，但仍然会涉及到大量的矩阵运算。在推断环节中，除了使用CPU或 [$GPU#ai_product*] 进行运算外，FPGA 以及 [$ASIC#ai_product*] 均能发挥重大作用。 （3）[$AI 芯片#ai_product*]按应用场景分类 主要分为用于服务器端（云端）和用于移动端（终端）两大类。 服务器端：在[$深度学习#ai_tec*]的训练阶段，由于数据量及运算量巨大，单一处理器几乎不可能独立完成一个模型的训练过程，因此，负责[@AI 算法#ai_tec*]的芯片采用的是高性能计算的技术路线，一方面要支持尽可能多的网络结构以保证算法的正确率和泛化能力；另一方面必须支持浮点数运算；而且为了能够提升性能必须支持阵列式结构（即可以把多块芯片组成一个计算阵列以加速运算）。在推断阶段，由于训练出来的深度[$神经网络模型#ai_tec*]仍非常复杂，推断过程仍然属于计算密集型和存储密集型，可以选择部署在服务器端。 移动端（手机、[@智能家居#ai_product*]、[@无人车#ai_product*]等）：移动端[$AI 芯片#ai_product*]在设计思路上与服务器端[$AI 芯片#ai_product*]有着本质的区别。首先，必须保证很高的计算能效；其次，在高级辅助驾驶 ADAS等设备对实时性要求很高的场合，推断过程必须在设备本身完成，因此要求移动端设备具备足够的推断能力。而某些场合还会有低功耗、低延迟、低成本的要求，从而导致移动端的 [$AI芯片#ai_product*]多种多样。 1.2 [$AI芯片#ai_product*]发展历程 从[@图灵#Person*]的论文《计算机器与智能》和图灵测试，到最初级的神经元模拟单元——感知机，再到现在多达上百层的[@深度神经网络#ai_tec*]，人类对[$人工智能#ai_tec*]的探索从来就没有停止过。[@上世纪八十年代#Date*]，多层神经网络和反向传播算法的出现给[$人工智能#ai_tec*]行业点燃了新的火花。反向传播的主要创新在于能将信息输出和目标输出之间的误差通过多层网络往前一级迭代反馈，将最终的输出收敛到某一个目标范围之内。[@1989 #Date*]年[@贝尔实验室#Org*]成功利用[@反向传播算法#ai_tec*]，在[@多层神经网络#ai_tec*]开发了一个[@手写邮编识别器#ai_product*]。[@1998#Date*]年 [@YannLeCun#Person*] 和 [@YoshuaBengio#Person*] 发表了[@手写识别神经网络#ai_tec*]和反向传播优化相关的论文《Gradient-based learning applied to document recognition》， 开 创 了 [@卷 积 神 经 网 络#ai_tec*] 的 时 代 。此后，[$人工智能#ai_tec*]陷入了长时间的发展沉寂阶段，直到[@1997#Date*] 年 [$IBM #Org*]的[@深蓝战胜国际象棋大师#ai_product*]和[@2011#Date*] 年 [$IBM #Org*]的[@沃森智能系统#ai_product*]在 Jeopardy 节目中胜出，[$人工智能#ai_tec*]才又一次为人们所关注。[@2016#Date*] 年 [@Alpha Go#ai_product*] 击败韩国围棋九段职业选手，则标志着[$人工智能#ai_tec*]的又一波高