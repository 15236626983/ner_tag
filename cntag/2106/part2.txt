潮。从基础算法、底层硬件、工具框架到实际应用场景，现阶段的人工智能领域已经全面开花。 作为人工智能核心的底层硬件 AI 芯片，也同样经历了多次的起伏和波折，总体看来，AI 芯片的发展前后经历了四次大的变化，其发展历程如图 2 所示。 4 图 2 AI 芯片发展历程 （1）2007 年以前，AI芯片产业一直没有发展成为成熟的产业；同时由于当时算法、数据量等因素，这个阶段AI 芯片并没有特别强烈的市场需求，通用的 CPU芯片即可满足应用需要。 （2）随着高清视频、VR、AR 游戏等行业的发展，GPU产品取得快速的突破；同时人们发现GPU的并行计算特性恰好适应人工智能算法及大数据并行计算的需求，如 GPU 比之前传统的 CPU在深度学习算法的运算上可以提高几十倍的效率，因此开始尝试使用GPU进行人工智能计算。 （3）进入 2010 年后，云计算广泛推广，人工智能的研究人员可以通过云计算借助大量CPU 和GPU 进行混合运算，进一步推进了 AI 芯片的深入应用，从而催生了各类 AI 芯片的研发与应用。 （4）人工智能对于计算能力的要求不断快速地提升，进入 2015 年后，GPU 性能功耗比不高的特点使其在工作适用场合受到多种限制，业界开始研发针对人工智能的专用芯片，以期通过更好的硬件和芯片架构，在计算效率、能耗比等性能上得到进一步提升。 1.3 我国 AI 芯片发展情况 目前，我国的人工智能芯片行业发展尚处于起步阶段。长期以来，中国在CPU、GPU、DSP 处理器设计上一直处于追赶地位，绝大部分芯片设计企业依靠国外的 IP 核设计芯片，在自主创新上受到了极大的限制。然而，人工智能的兴起，无疑为中国在处理器领域实现5弯道超车提供了绝佳的机遇。人工智能领域的应用目前还处于面向行业应用阶段，生态上尚未形成垄断，国产处理器厂商与国外竞争对手在人工智能这一全新赛场上处在同一起跑线上，因此，基于新兴技术和应用市场，中国在建立人工智能生态圈方面将大有可为。 由于我国特殊的环境和市场，国内 AI 芯片的发展目前呈现出百花齐放、百家争鸣的态势，AI 芯片的应用领域也遍布股票交易、金融、商品推荐、安防、早教机器人以及无人驾驶等众多领域，催生了大量的人工智能芯片创业公司，如地平线、深鉴科技、中科寒武纪等。 尽管如此，国内公司却并未如国外大公司一样形成市场规模，反而出现各自为政的散裂发展现状。除了新兴创业公司，国内研究机构如北京大学、清华大学、中国科学院等在AI 芯片领域都有深入研究；而其他公司如百度和比特大陆等，2017 年也有一些成果发布。 可以预见，未来谁先在人工智能领域掌握了生态系统，谁就掌握住了这个产业的主动权。 6ASIC 芯片非常适合人工智能的应用场景。首先，ASIC的性能提升非常明显。例如英伟达首款专门为深度学习从零开始设计的芯片TeslaP100 数据处理速度是其 2014 年推出GPU系列的 12 倍。谷歌为机器学习定制的芯片TPU 将硬件性能提升至相当于当前芯片按摩尔定律发展 7 年后的水平。正如 CPU改变了当年庞大的计算机一样，人工智能 ASIC 芯片也将大幅改变如今 AI 硬件设备的面貌。如大名鼎鼎的 AlphaGo 使用了约170 个图形处理器（GPU）和1200 个中央处理器（CPU），这些设备需要占用一个机房，还要配备大功率的空调，以及多名专家进行系统维护。而如果全部使用专用芯片，极大可能只需要一个普通收纳盒大小的空间，，且功耗也会大幅降低。 第二，下游需求促进人工智能芯片专用化。从服务器，计算机到无人驾驶汽车、无人机再到智能家居的各类家电，至少数十倍于智能手机体量的设备需要引入感知交互能力和人工智能计算能力。而出于对实时性的要求以及训练数据隐私等考虑，这些应用不可能完全依赖云端，必须要有本地的软硬件基础平台支撑，这将带来海量的人工智能芯片需求。 目前人工智能专用芯片的发展方向包括：主要基于FPGA的半定制、针对深度学习算法的全定制和类脑计算芯片3 个方向。 在芯片需求还未形成规模、深度学习算法暂未稳定，AI 芯片本身需要不断迭代改进的情况下，利用具备可重构特性的 FPGA 芯片来实现半定制的人工智能芯片是最佳选择之一。这类芯片中的杰出代表是国内初创公司深鉴科技，该公司设计了“深度学习处理单元”（Deep Processing Unit，DPU）的芯片，希望以 ASIC 级别的功耗达到优于 GPU 的性能，其第一批产品就是基于 FPGA 平台开发研制出来的。这种半定制芯片虽然依托于FPGA 平台，但是抽象出了指令集与编译器，可以快速开发、快速迭代，与专用的FPGA 加速器产品相比，也具有非常明显的优势。 深度学习算法稳定后，AI芯片可采用ASIC 设计方法进行全定制，使性能、功耗和面积等指标面向深度学习算法做到最优。 2.5 类脑芯片 类脑芯片不采用经典的冯·诺依曼架构，而是基于神经形态架构设计，以IBM Truenorth 为代表。IBM研究人员将存储单元作为突触、计算单元作为神经元、传输单元作为轴突搭建了神经芯片的原型。目前，Truenorth用三星 28nm 功耗工艺技术，由54 亿个晶体管组成的芯片构成的片上网络有 4096 个神经突触核心，实时作业功耗仅为 70mW。由于神经突触要求权重可变且要有记忆功能，IBM 采用与 CMOS工艺兼容的相变非挥发存储器（PCM）的技术实验性的实现了新型突触，加快了商业化进程。 在国内，清华大学类脑计算中心于 2015 年11 月成功的研制了国内首款超大规模的神经形态类脑计算天机芯片。该芯片同时支持脉冲神经网络和人工神经网络（深度神经网络），13可进行大规模神经元网络的模拟。中心还开发了面向类脑芯片的工具链，降低应用的开发难度并提升效率。第二代 28nm 天机芯片也已问世，在性能功耗比上要优于Truenorth。 当前，类脑 AI 芯片的设计目的不再仅仅局限于加速深度学习算法，而是在芯片基本结构甚至器件层面上改变设计，希望能够开发出新的类脑计算机体系结构，比如采用忆阻器和ReRAM 等新器件来提高存储密度。这类芯片技术尚未完全成熟，离大规模应用还有一定的差距，但是长期来看类脑芯片有可能会带来计算机体系结构的革命。 2.6 AI芯片技术特点比较 通过以上分析，我们可以总结出以下几个特点。 l CPU 通用性最强，但延迟严重，散热高，效率最低。 l GPU通用性强、速度快、效率高，特别适合用在深度学习训练方面，但是性能功耗比较低。 l FPGA具有低能耗、高性能以及可编程等特性，相对于CPU 与 GPU有明显的性能或者能耗优势，但对使用者要求高。 l ASIC 可以更有针对性地进行硬件层次的优化，从而获得更好的性能、功耗比。但是ASIC 芯片的设计和制造需要大量的资金、较长的研发周期和工程周期，而且深度学习算法仍在快速发展，若深度学习算法发生大的变化，FPGA 能很快改变架构，适应最新的变化，ASIC类芯片一旦定制则难于进行修改。 当前阶段，GPU配合 CPU 仍然是AI 芯片的主流，而后随着视觉、语音、深度学习的算法在FPGA 以及ASIC 芯片上的不断优化，此两者也将逐步占有更多的市场份额，从而与GPU达成长期共存的局面。从长远看，人工智能类脑神经芯片是发展的路径和方向。 143 产业篇 本篇将介绍目前人工智能芯片技术领域的国