The [@artificial intelligence#ai_tec*] community needs to put a lot more effort and money into systems protection to avoid it costing billions more down the line, according to the founding director of the [@Center for Security and Emerging Technology#Org*]. [@Jason Matheny#Person*] leads the center, a shop housed at [@Georgetown University#MISC*]'s [@Walsh School#MISC*] of Foreign Service to provide policy and intelligence analysis to the government. Prior to launching the think tank, [@Matheny#Person*] was assistant director of national intelligence, and before that he served as director of [@IARPA#ai_program*] â€” the [@Intelligence Advanced Research Projects Activity#ai_program*]. He currently sits on the [@National Security Commission on Artificial Intelligence#Org*]. Speaking from that vantage [@Sept. 4#Date*] during a panel at the [@2019 Intelligence and National Security Summit#MISC*], [@Matheny#Person*] said [@AI#ai_tec*] systems are not being developed with a healthy focus on evolving threats, despite increased funding by the [@Pentagon#Org*] and the private sector. "A lot of the techniques that are used today were built without intelligent adversaries in mind. They were sort of innocent." For [@Matheny#Person*], there are three main types of attacks developers need to consider: [@adversarial examples#ai_tec*], [@trojans#ai_tec*] and [@model inversion#ai_tec*]. [@Adversarial examples#ai_tec*] are attempts to confuse [@AI#ai_tec*] systems by tricking it into misclassifying data. By exploiting the ways an [@AI#ai_tec*] system processes data, an adversary can trick it into seeing something that isn't there. "[It's] a technique that an adversary can use to confuse a classifier into thinking that, for instance, an image of a tank is instead an image of a school bus or a porpoise. It's a way of creating sort of an optical illusion for a [@machine-learning#ai_tec*] system," explained [@Matheny#Person*]. While [@adversarial examples#ai_tec*] are generally used on fully developed [@AI#ai_tec*] systems, trojan attacks are used during [@AI#ai_tec*] development. In a trojan attack, an adversary can introduce a change in the environment in which the system is learning, which causes it to learn the wrong lesson. A third type of attack, called [@model inversion#ai_tec*], is used on [@machine-learning#ai_tec*] systems. With [@model inversion#ai_tec*], adversaries basically reverse-engineer the machine learning in order to see the information that was used to train it. "So if you have a bunch of classified data that is going into a model, you really should be protecting the model even after it's been trained at the highest level of classification of the data used to train it," said [@Matheny#Person*]. "Please be careful with your trained models." Despite these three vulnerabilities, [@Matheny#Person*] noted that less than one percent of [@AI#ai_tec*] research and development funding is going toward [@AI#ai_tec*] security. "In some of the same ways that we're now retrofitting most IT systems with security measures that are meant to address vulnerabilities that were baked into systems in the [@1980s#Date*] or earlier, in some cases we're going to have to start baking in security from the start with [@AI#ai_tec*], unless we want to spend billions of dollars retrofitting security years from now," said [@Matheny#Person*].