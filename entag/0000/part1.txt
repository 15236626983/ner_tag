AI technologies have been on the US Department of Defense (DoD) radar for decades. Moreover, separate branches of the military (army, navy and air force) have each published on the use of AI in their respective domains. This report will focus on the general DoD strategy. The key point of reference is the 2014 ‘Third Offset Strategy’, which seeks to outmanoeuvre advantages made by top adversaries through technology. As the then Deputy Secretary of Defence Bob Work put it in 2016: “We believe quite strongly that the technological sauce of the Third Offset is going to be advances in Artificial Intelligence (AI) and autonomy”. According to him the Third Offset’s aim “is to exploit all advances in artificial intelligence and autonomy and insert them into DoD’s battle networks to achieve a step increase in performance that the department believes will strengthen conventional deterrence”. The abovementioned 2016 report ‘Preparing for the Future of Artificial Intelligence’ also refers to the weaponisation of AI: “Given advances in military technology and artificial intelligence more broadly, scientists, strategists, and military experts all agree that the future of LAWS is difficult to predict and the pace of change is rapid. Many new capabilities may soon be possible, and quickly able to be developed and operationalized. The Administration is engaged in active, ongoing interagency discussions to work toward a government- wide policy on autonomous weapons consistent with shared human values, national security interests, and international and domestic obligations.” In August 2018, a Pentagon strategy report noted that the “technologies underpinning unmanned systems would make it possible to develop and deploy autonomous systems that could independently select and attack targets with lethal force” but that commanders were reluctant to surrender control to such systems, in part due to lack of confidence in the machine-learning system. That is why one of the numerous AI programmes that DARPA is working on is the Explainable AI programme, which aims to create machine-learning techniques that produce more explainable models “while maintaining a high level of learning performance”, and enable human users “to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners”. A day after the White House’s American AI Initiative, the DoD released its AI strategy, which calls for the rapid deployment of “resilient, robust, reliable, and secure” AI-enabled technologies to “address key missions” across the DoD”.  This strategy puts the Joint Artificial Intelligence Centre (JAIC) at the forefront of efforts, focusing on collaborations with the private sector and academia. Again, this AI strategy is unclear about how its implementation will be funded. The US is one of the very few states to have a policy specifically on lethal autonomous weapon systems. In its 2012 3000.09 Directive, the DoD states that “semi-autonomous weapon systems that are onboard or integrated with unmanned platforms must be designed such that, in the event of degraded or lost communications, the system does not autonomously select and engage individual targets or specific target groups that have not been previously selected by an authorized human operator”.  The regulation refers to “human-supervised autonomous weapons systems” that are limited to military purposes, prohibits the “selecting of humans as targets” and allows for computer- controlled non-lethal systems. General Paul Selva, the second-highest-ranking military officer in the US, said in 2016 that the US would have the technology within a decade to build an autonomous system that could decide on its own who and when to kill, but added that the US has no intention of building one. That same year, then Deputy Secretary of Defense Bob Work also confirmed that when it comes to decisions over life and death, “there will always be a man in the loop”. However, there is a loophole in the Directive: any use of autonomous or semi-autonomous systems that falls outside its scope must be approved by three top Pentagon officials. But what they consider as “appropriate levels of human judgment in the use of force” is left undefined.  Also the term ‘human in the loop’ does not appear anywhere in the directive. “The Directive does not use the phrase ‘human in the loop,’ so we recommend not indicating that DoD has established requirements using that term,” according to a DoD spokesperson. There are many different DoD programmes and initiatives looking at military applications of AI, as well as more specifically at autonomous weapon systems. According to DARPA itself, it “has played a leading role in the creation and advancement of artificial intelligence (AI) technologies that have produced game-changing capabilities for the Department of Defense” over the past 60 years. To stay ahead of others, especially China, the US military has increased its commitment. In September 2018, the Pentagon pledged to make the largest investment to date in AI systems for US weaponry, committing to spend USD 2 billion over the next five years through DARPA to “develop [the] next wave of AI technologies”. One example is DARPA’s Collaborative Operations in Denied Environment (CODE) programme. DARPA points out that most current unmanned aerial systems require “continuous control by a dedicated pilot and sensor operator supported by numerous telemetry-linked analysts”. Hence, the CODE programme aims to develop new algorithms or software “for existing unmanned aircraft that would extend mission capabilities and improve U.S. forces’ ability to conduct operations in denied or contested airspace”. In addition, “using collaborative autonomy, CODE-enabled unmanned aircraft would find targets and engage them as appropriate under established rules of engagement, leverage nearby CODE-equipped systems with minimal supervision, and adapt to dynamic situations such as attrition of friendly forces or the emergence of unanticipated threats”.  Testing was undertaken by arms producers Lockheed Martin and Raytheon. It was reported in March 2019 that a Pentagon project may lead to the world’s “first large-scale armed unmanned warship”. The Overlord programme “will develop core autonomy” and field prototype unmanned surface vessels “capable of being seamlessly operable with the fleet”. Another example is the army’s Advanced Targeting and Lethality Automated System (ATLAS), which “will use artificial intelligence and machine learning to give ground-combat vehicles autonomous target capabilities” that will allow weapons to “acquire, identify, and engage targets at least 3X faster than the current manual process”.  Still, it appears that a human makes the final decision to attack a target. The United States DoD recognises that expertise in artificial intelligence lies with the private sector, and specifically tech companies and research institutes. In this section we look at the initiatives the Pentagon has undertaken to stimulate cooperation, the challenges involved in that cooperation, and examples of cooperation with the private sector. Acknowledging the innovative power of the private sector, the DoD is keen to have better connections with the engineers in Silicon Valley. Indeed, recent initiatives demonstrate that public-private partnership is a US military AI priority. One such initiative is the Defense Innovation Unit Experimental (DIUx), set up in 2015 and “meant to serve as a liaison between the Defence Department and the tech world”.  The DIUx contracts companies “offering solutions in a variety of areas—from autonomy and AI to human systems, IT, and space—to solve a host of defence problems”.  The DIUx was set up initially as an experiment, but in August 2018 the DoD announced that it would be renamed the Defense Innovation Unit (DIU) “to convey a sense of permanence to the agency”. Establishing collaboration with private companies can be challenging as well, as the widely publicised case of Google and Project Maven has shown. Launched in April 2017, the objective of Project Maven is to “turn the enormous volume of data available to the DoD into actionable intelligence and insights at speed”.  To do so, “the project aims to develop and integrate ‘computer-vision algorithms needed to help the military and civilian analysts encumbered by the sheer volume of full-motion video data that DoD collects every day in support of counterinsurgency and counterterrorism operations,’ according to the Pentagon”.  The project was known for its collaboration with Google. However, following protests from Google employees, Google stated that it would not renew its contract.  Nevertheless, other tech companies such as Clarifai, Amazon and Microsoft still collaborate with the Pentagon on this project. The Project Maven controversy deepened the gap between the AI community and the Pentagon. To bridge it, two new initiatives have been developed.  One is the creation of the aforementioned JAIC with the goal of “accelerating the delivery of AI-enabled capabilities, scaling the Department- wide impact of AI, and synchronizing DoD AI activities to expand Joint Force advantages”, by “collaborating within DoD, across government, and with industry, academia, and US allies to strengthen partnerships, highlight critical needs, solve problems of urgent operational significance, and adapt AI technologies for DoD missions”. As a result of this controversy, the DoD is working on a new review of AI ethics through the Defense Innovation Board (DIB). It aims to develop principles for the use of AI by the military, “particularly while the adoption of this technology is at a nascent stage”.  According to the DIB, “these AI Principles should demonstrate DoD’s commitment to deter war and use AI responsibly to ensure civil liberties and the rule of law are protected”. At the same time, there is a long history of tech sector cooperation through DARPA programmes. One recent example is the OFFSET programme (OFFensive Swarm-Enabled Tactics), with the aim of “using swarms compromising upwards of 250 unmanned aircraft systems (UASs) and/or unmanned ground systems (UGSs) to accomplish diverse missions in complex urban environments”. This programme is being undertaken in collaboration with Carnegie Mellon University, Cornell University, Michigan Technological University and others, as well as with start-ups such as Corenova Technologies, Inc. Another programme is the Squad X Experimentation Programme,  which is exploring four key technical areas: precision engagement, non-kinetic engagement, squad sensing and squad autonomy.  The aim of the programme is for human fighters to “have a greater sense of confidence in their autonomous partners, as well as a better understanding of how the autonomous systems would likely act on the battlefield”,  as well as to “extend and enhance the situational awareness of small, dismounted units”.  In this programme, Lockheed Martin Missiles is working on approaches to “provide unique capabilities to enhance ground infantries”. One of the most publicised programmes is the Joint Enterprise Defense Infrastructure (JEDI), aiming to use “commercial cloud services to transform how DoD captures, processes, understands, and harnesses its data to deliver advanced capabilities, enable real-time decision-making, and support joint force operations”.  It has been reported that “the real force driving Jedi is the desire to weaponize AI—what the defence department has been calling ‘algorithmic warfare’. By pooling the military’s data into a modern cloud platform, and using the machine-learning services that such platforms provide to analyse the data, JEDI will help the Pentagon realize its AI ambitions”.  The JEDI contract is reportedly worth USD 10 billion,  and many big tech companies have submitted bids, including Microsoft, Oracle and IBM.  Amazon is believed to be the main contender. DARPA also has the Gremlins programme. The programme “envisions launching groups of UASs from existing large aircraft such as bombers or transport aircraft […] while those planes are out of range of adversary defences”. The rationale is that being able to send larger numbers of UASs “with coordinated, distributed capabilities” could provide the US with better operational flexibility at a much lower cost.  In May 2018, it was announced that the Phase III contract had been awarded to Dynetics. In February 2019, it was announced that the DoD is launching the US Army’s Artificial Intelligence Task Force in collaboration with Carnegie Mellon University (CMU). The location of this task force will allow the army to work closely with CMU as well as other universities and companies in the Pittsburgh region.  The DoD is investing USD 72 million in the five-year effort. “Tackling difficult science and technology challenges is rarely done alone and there is no greater challenge or opportunity facing the Army than Artificial Intelligence,” said the director of the army’s corporate laboratory.