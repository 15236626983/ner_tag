[@AI#ai_tec*] technologies have been on the [@US#Gpe*] [@Department of Defense#Org*] ([@DoD#Org*]) radar for decades. Moreover, separate branches of the military (army, navy and air force) have each published on the use of [@AI#ai_tec*] in their respective domains. This report will focus on the general [@DoD#Org*] strategy. The key point of reference is the [@2014#Date*] ‘[@Third Offset Strategy#ai_program*]’, which seeks to outmanoeuvre advantages made by top adversaries through technology. As the then Deputy Secretary of [@Defence#Org*] [@Bob Work#Person*] put it in [@2016#Date*]: “We believe quite strongly that the technological sauce of the [@Third Offset#ai_program*] is going to be advances in [@Artificial Intelligence#ai_tec*] ([@AI#ai_tec*]) and [@autonomy#ai_tec*]”. According to him the [@Third Offset#ai_program*]’s aim “is to exploit all advances in [@artificial intelligence#ai_tec*] and [@autonomy#ai_tec*] and insert them into [@DoD#Org*]’s battle networks to achieve a step increase in performance that the department believes will strengthen conventional deterrence”. The abovementioned [@2016#Date*] report ‘Preparing for the Future of [@Artificial Intelligence#ai_tec*]’ also refers to the weaponisation of [@AI#ai_tec*]: “Given advances in military technology and [@artificial intelligence#ai_tec*] more broadly, scientists, strategists, and military experts all agree that the future of LAWS is difficult to predict and the pace of change is rapid. Many new capabilities may soon be possible, and quickly able to be developed and operationalized. [@The Administration#Org*] is engaged in active, ongoing interagency discussions to work toward a government- wide policy on [@autonomous weapons#ai_product*] consistent with shared human values, national security interests, and international and domestic obligations.” In [@August 2018#Date*], a [@Pentagon#Org*] strategy report noted that the “technologies underpinning [@unmanned systems#ai_product*] would make it possible to develop and deploy [@autonomous systems#ai_product*] that could independently select and attack targets with lethal force” but that commanders were reluctant to surrender control to such systems, in part due to lack of confidence in the [@machine-learning system#ai_product*]. That is why one of the numerous [@AI#ai_tec*] programmes that [@DARPA#Org*] is working on is the [@Explainable AI#ai_tec*] programme, which aims to create [@machine-learning#ai_tec*] techniques that produce more [@explainable models#ai_tec*] “while maintaining a high level of learning performance”, and enable human users “to understand, appropriately trust, and effectively manage the emerging generation of [@artificially intelligent#ai_tec*] partners”. A day after the [@White House#Org*]’s [@American AI Initiative#ai_program*], the [@DoD#Org*] released its [@AI#ai_tec*] strategy, which calls for the rapid deployment of “resilient, robust, reliable, and secure” [@AI-enabled#ai_tec*] technologies to “address key missions” across the [@DoD#Org*]”.  This strategy puts the [@Joint Artificial Intelligence Centre#Org*] ([@JAIC#Org*]) at the forefront of efforts, focusing on collaborations with the private sector and academia. Again, this [@AI#ai_tec*] strategy is unclear about how its implementation will be funded. The [@US#Gpe*] is one of the very few states to have a policy specifically on lethal [@autonomous weapon systems#ai_product*]. In its [@2012#Date*] [@3000.09 Directive#ai_program*], the [@DoD#Org*] states that “[@semi-autonomous weapon systems#ai_product*] that are onboard or integrated with [@unmanned platforms#ai_product*] must be designed such that, in the event of degraded or lost communications, the system does not autonomously select and engage individual targets or specific target groups that have not been previously selected by an authorized human operator”.  The regulation refers to “human-supervised [@autonomous weapons systems#ai_product*]” that are limited to military purposes, prohibits the “selecting of humans as targets” and allows for computer- controlled non-lethal systems. General [@Paul Selva#Person*], the second-highest-ranking military officer in the [@US#Gpe*], said in [@2016#Date*] that the [@US#Gpe*] would have the technology within a decade to build an [@autonomous system#ai_product*] that could decide on its own who and when to kill, but added that the [@US#Gpe*] has no intention of building one. That same year, then Deputy Secretary of [@Defense#Org*] [@Bob Work#Person*] also confirmed that when it comes to decisions over life and death, “there will always be a man in the loop”. However, there is a loophole in the Directive: any use of [@autonomous or semi-autonomous systems#ai_product*] that falls outside its scope must be approved by three top [@Pentagon#Org*] officials. But what they consider as “appropriate levels of human judgment in the use of force” is left undefined.  Also the term ‘human in the loop’ does not appear anywhere in the directive. “The Directive does not use the phrase ‘human in the loop,’ so we recommend not indicating that [@DoD#Org*] has established requirements using that term,” according to a [@DoD#Org*] spokesperson. There are many different [@DoD#Org*] programmes and initiatives looking at [@military applications of AI#ai_product*], as well as more specifically at [@autonomous weapon systems#ai_product*]. According to [@DARPA#Org*] itself, it “has played a leading role in the creation and advancement of [@artificial intelligence#ai_tec*] ([@AI#ai_tec*]) technologies that have produced game-changing capabilities for the [@Department of Defense#Org*]” [@over the past 60 years#Date*]. To stay ahead of others, especially [@China#Gpe*], the [@US#Gpe*] military has increased its commitment. In [@September#Date*] [@2018#Date*], the [@Pentagon#Org*] pledged to make the largest investment to date in [@AI systems#ai_product*] for [@US#Gpe*] weaponry, committing to spend USD 2 billion [@over the next five years#Date*] through [@DARPA#Org*] to “develop [the] next wave of [@AI#ai_tec*] technologies”. One example is [@DARPA#Org*]’s [@Collaborative Operations in Denied Environment#ai_program*] ([@CODE#ai_program*]) programme. [@DARPA#Org*] points out that most current [@unmanned aerial systems#ai_product*] require “continuous control by a dedicated pilot and sensor operator supported by numerous telemetry-linked analysts”. Hence, the [@CODE#ai_program*] programme aims to develop new algorithms or software “for existing [@unmanned aircraft#ai_product*] that would extend mission capabilities and improve [@U.S.#Gpe*] forces’ ability to conduct operations in denied or contested airspace”. In addition, “using collaborative [@autonomy#ai_tec*], [@CODE#ai_program*]-enabled [@unmanned aircraft#ai_product*] would find targets and engage them as appropriate under established rules of engagement, leverage nearby [@CODE#ai_program*]-equipped systems with minimal supervision, and adapt to dynamic situations such as attrition of friendly forces or the emergence of unanticipated threats”.  Testing was undertaken by arms producers [@Lockheed Martin#Person*] and [@Raytheon#Person*]. It was reported in [@March 2019#Date*] that a [@Pentagon#Org*] project may lead to the world’s “first large-scale armed [@unmanned warship#ai_product*]”. The [@Overlord programme#ai_program*] “will develop core [@autonomy#ai_tec*]” and field prototype [@unmanned surface vessels#ai_product*] “capable of being seamlessly operable with the fleet”. Another example is the army’s [@Advanced Targeting and Lethality Automated System#ai_program*] ([@ATLAS#ai_program*]), which “will use [@artificial intelligence#ai_tec*] and [@machine learning#ai_tec*] to give ground-combat vehicles [@autonomous target#ai_product*] capabilities” that will allow weapons to “acquire, identify, and engage targets at least 3X faster than the current manual process”.  Still, it appears that a human makes the final decision to attack a target. The [@United States#Gpe*] [@DoD#Org*] recognises that expertise in [@artificial intelligence#ai_tec*] lies with the private sector, and specifically tech companies and research institutes. In this section we look at the initiatives the [@Pentagon#Org*] has undertaken to stimulate cooperation, the challenges involved in that cooperation, and examples of cooperation with the private sector. Acknowledging the innovative power of the private sector, the [@DoD#Org*] is keen to have better connections with the engineers in [@Silicon Valley#Loc*]. Indeed, recent initiatives demonstrate that public-private partnership is a [@US#Gpe*] military [@AI#ai_tec*] priority. One such initiative is the [@Defense Innovation Unit Experimental#Org*] ([@DIUx#Org*]), set up in [@2015#Date*] and “meant to serve as a liaison between the [@Defence Department#Org*] and the tech world”.  The [@DIUx#Org*] contracts companies “offering solutions in a variety of areas—from [@autonomy#ai_tec*] and [@AI#ai_tec*] to human systems, IT, and space—to solve a host of defence problems”.  The [@DIUx#Org*] was set up initially as an experiment, but in [@August 2018#Date*] the [@DoD#Org*] announced that it would be renamed the [@Defense Innovation Unit#Org*] ([@DIU#Org*]) “to convey a sense of permanence to the agency”. Establishing collaboration with private companies can be challenging as well, as the widely publicised case of [@Google#Org*] and [@Project Maven#ai_program*] has shown. Launched in [@April 2017#Date*], the objective of [@Project Maven#ai_program*] is to “turn the enormous volume of data available to the [@DoD#Org*] into actionable intelligence and insights at speed”.  To do so, “the project aims to develop and integrate ‘[@computer-vision#ai_tec*] algorithms needed to help the military and civilian analysts encumbered by the sheer volume of full-motion video data that [@DoD#Org*] collects every day in support of counterinsurgency and counterterrorism operations,’ according to the [@Pentagon#Org*]”.  The project was known for its collaboration with [@Google#Org*]. However, following protests from [@Google#Org*] employees, [@Google#Org*] stated that it would not renew its contract.  Nevertheless, other tech companies such as [@Clarifai#Org*], [@Amazon#Org*] and [@Microsoft#Org*] still collaborate with the [@Pentagon#Org*] on this project. The [@Project Maven#ai_program*] controversy deepened the gap between the [@AI community#Org*] and the [@Pentagon#Org*]. To bridge it, two new initiatives have been developed.  One is the creation of the aforementioned [@JAIC#Org*] with the goal of “accelerating the delivery of [@AI-enabled#ai_tec*] capabilities, scaling the Department- wide impact of [@AI#ai_tec*], and synchronizing [@DoD#Org*] [@AI#ai_tec*] activities to expand Joint Force advantages”, by “collaborating within [@DoD#Org*], across government, and with industry, academia, and [@US#Gpe*] allies to strengthen partnerships, highlight critical needs, solve problems of urgent operational significance, and adapt [@AI#ai_tec*] technologies for [@DoD#Org*] missions”. As a result of this controversy, the [@DoD#Org*] is working on a new review of [@AI#ai_tec*] ethics through the [@Defense Innovation Board#ai_program*] ([@DIB#ai_program*]). It aims to develop principles for the use of [@AI#ai_tec*] by the military, “particularly while the adoption of this technology is at a nascent stage”.  According to the [@DIB#ai_program*], “these [@AI Principles#ai_program*] should demonstrate [@DoD#Org*]’s commitment to deter war and use [@AI#ai_tec*] responsibly to ensure civil liberties and the rule of law are protected”. At the same time, there is a long history of tech sector cooperation through [@DARPA#Org*] programmes. One recent example is the [@OFFSET#ai_program*] programme ([@OFFensive Swarm-Enabled Tactics#ai_program*]), with the aim of “using swarms compromising upwards of 250 [@unmanned aircraft systems#ai_product*] ([@UASs#ai_product*]) and/or [@unmanned ground systems#ai_product*] ([@UGSs#ai_product*]) to accomplish diverse missions in complex urban environments”. This programme is being undertaken in collaboration with [@Carnegie Mellon University#Org*], [@Cornell University#Org*], [@Michigan Technological University#Org*] and others, as well as with start-ups such as [@Corenova Technologies#Org*], Inc. Another programme is the [@Squad X Experimentation Programme#ai_program*],  which is exploring four key technical areas: precision engagement, non-kinetic engagement, [@squad sensing#ai_product*] and [@squad autonomy#ai_product*].  The aim of the programme is for human fighters to “have a greater sense of confidence in their [@autonomous partners#ai_product*], as well as a better understanding of how the [@autonomous systems#ai_product*] would likely act on the battlefield”,  as well as to “extend and enhance the situational awareness of small, dismounted units”.  In this programme, [@Lockheed Martin Missiles#Org*] is working on approaches to “provide unique capabilities to enhance ground infantries”. One of the most publicised programmes is the [@Joint Enterprise Defense Infrastructure#ai_program*] ([@JEDI#ai_program*]), aiming to use “commercial cloud services to transform how [@DoD#Org*] captures, processes, understands, and harnesses its data to deliver advanced capabilities, enable real-time decision-making, and support joint force operations”.  It has been reported that “the real force driving Jedi is the desire to weaponize [@AI#ai_tec*]—what the defence department has been calling ‘[@algorithmic warfare#ai_product*]’. By pooling the military’s data into a modern cloud platform, and using the [@machine-learning#ai_tec*] services that such platforms provide to analyse the data, [@JEDI#ai_program*] will help the [@Pentagon#Org*] realize its [@AI#ai_tec*] ambitions”.  The [@JEDI#ai_program*] contract is reportedly worth [$US#Gpe*]D 10 billion,  and many big tech companies have submitted bids, including [@Microsoft#Org*], [@Oracle#Org*] and [@IBM#Org*].  [@Amazon#Org*] is believed to be the main contender. [@DARPA#Org*] also has the [@Gremlins programme#ai_program*]. The programme “envisions launching groups of [@UASs#ai_product*] from existing large aircraft such as bombers or transport aircraft […] while those planes are out of range of adversary defences”. The rationale is that being able to send larger numbers of [@UASs#ai_product*] “with coordinated, distributed capabilities” could provide the [$US#Gpe*] with better operational flexibility at a much lower cost.  In [@May 2018#Date*], it was announced that the Phase III contract had been awarded to [@Dynetics#Org*]. In [@February 2019#Date*], it was announced that the [@DoD#Org*] is launching the [@US Army’s Artificial Intelligence Task Force#ai_program*] in collaboration with [@Carnegie Mellon University#Org*] ([@CMU#Org*]). The location of this task force will allow the army to work closely with [@CMU#Org*] as well as other universities and companies in the [@Pittsburgh#Gpe*] region.  The [@DoD#Org*] is investing [$US#Gpe*]D 72 million in the five-year effort. “Tackling difficult science and technology challenges is rarely done alone and there is no greater challenge or opportunity facing the Army than [@Artificial Intelligence#ai_tec*],” said the director of the army’s corporate laboratory.