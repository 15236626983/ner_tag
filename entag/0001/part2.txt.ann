[@AI#ai_tec*] is likely to be a key technology in advancing military cyber operations. In his [@2016#Date*] testimony before the [@Senate Armed Services Committee#Org*], Commander of [@U.S. Cyber Command#Org*] Admiral [@Michael Rogers#Person*] stated that relying on human intelligence alone in cyberspace is “a losing strategy.” He later clarified this point, stating, “If you can’t get some level of [$AI#ai_tec*] or [@machine learning#ai_tec*] with the volume of activity you’re trying to understand when you’re defending networks ... you are always behind the power curve.” Conventional cybersecurity tools look for historical matches to known malicious code, so hackers only have to modify small portions of that code to circumvent the defense. [@AI-enabled#ai_tec*] tools, on the other hand, can be trained to detect anomalies in broader patterns of network activity, thus presenting a more comprehensive and dynamic barrier to attack. [@DARPA#Org*]’s [$2016#Date*] [@Cyber Grand Challenge#ai_program*] demonstrated the potential power of [$AI-enabled#ai_tec*] cyber tools. The competition challenged participants to develop [$AI#ai_tec*] algorithms that could autonomously “detect, evaluate, and patch software vulnerabilities before [competing teams] have a chance to exploit them”—all within a matter of seconds, rather than the usual months. The challenge demonstrated not only the potential speed of [$AI-enabled#ai_tec*] cyber tools but also the potential ability of a singular algorithm to play offense and defense simultaneously. These capabilities could provide a distinct advantage in future cyber operations. [$AI#ai_tec*] is enabling increasingly realistic photo, audio, and video forgeries, or “deep fakes,” that adversaries could deploy as part of their information operations. Indeed, [@deep fake technology#ai_tec*] could be used against the [@United States#Gpe*] and [@U.S. allies#Gpe*] to generate false news reports, influence public discourse, erode public trust, and attempt to blackmail diplomats. Although most previous deep fakes have been detectable by experts, the sophistication of the technology is progressing to the point that it may soon be capable of fooling forensic analysis tools. In order to combat [@deep fake technologies#ai_tec*], [$DARPA#Org*] has launched the [@Media Forensics#ai_program*] ([@MediFor#ai_program*]) project, which seeks to “automatically detect manipulations, provide detailed information about how these manipulations were performed, and reason about the overall integrity of visual media.” [$MediFor#ai_program*] has developed some initial tools for identifying [@AI-produced#ai_tec*] forgeries, but as one analyst has noted, “a key problem … is that [@machine-learning systems#ai_product*] can be trained to outmaneuver forensics tools.” For this reason, [$DARPA#Org*] plans to host follow-on contests to ensure that forensic tools keep pace with [$deep fake technologies#ai_tec*]. [@Artificial intelligence#ai_tec*] could also be used to create full “[@digital patterns-of-life#ai_product*],” in which an individual’s digital “footprint” is “merged and matched with purchase histories, credit reports, professional resumes, and subscriptions” to create a comprehensive behavioral profile of servicemembers, suspected intelligence officers, government officials, or private citizens. As in the case of [@deep fakes#ai_tec*], this information could, in turn, be used for targeted influence operations or blackmail.