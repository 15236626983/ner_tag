AI is likely to be a key technology in advancing military cyber operations. In his 2016 testimony before the Senate Armed Services Committee, Commander of U.S. Cyber Command Admiral Michael Rogers stated that relying on human intelligence alone in cyberspace is “a losing strategy.” He later clarified this point, stating, “If you can’t get some level of AI or machine learning with the volume of activity you’re trying to understand when you’re defending networks ... you are always behind the power curve.” Conventional cybersecurity tools look for historical matches to known malicious code, so hackers only have to modify small portions of that code to circumvent the defense. AI-enabled tools, on the other hand, can be trained to detect anomalies in broader patterns of network activity, thus presenting a more comprehensive and dynamic barrier to attack. DARPA’s 2016 Cyber Grand Challenge demonstrated the potential power of AI-enabled cyber tools. The competition challenged participants to develop AI algorithms that could autonomously “detect, evaluate, and patch software vulnerabilities before [competing teams] have a chance to exploit them”—all within a matter of seconds, rather than the usual months. The challenge demonstrated not only the potential speed of AI-enabled cyber tools but also the potential ability of a singular algorithm to play offense and defense simultaneously. These capabilities could provide a distinct advantage in future cyber operations. AI is enabling increasingly realistic photo, audio, and video forgeries, or “deep fakes,” that adversaries could deploy as part of their information operations. Indeed, deep fake technology could be used against the United States and U.S. allies to generate false news reports, influence public discourse, erode public trust, and attempt to blackmail diplomats. Although most previous deep fakes have been detectable by experts, the sophistication of the technology is progressing to the point that it may soon be capable of fooling forensic analysis tools. In order to combat deep fake technologies, DARPA has launched the Media Forensics (MediFor) project, which seeks to “automatically detect manipulations, provide detailed information about how these manipulations were performed, and reason about the overall integrity of visual media.” MediFor has developed some initial tools for identifying AI-produced forgeries, but as one analyst has noted, “a key problem … is that machine-learning systems can be trained to outmaneuver forensics tools.” For this reason, DARPA plans to host follow-on contests to ensure that forensic tools keep pace with deep fake technologies. Artificial intelligence could also be used to create full “digital patterns-of-life,” in which an individual’s digital “footprint” is “merged and matched with purchase histories, credit reports, professional resumes, and subscriptions” to create a comprehensive behavioral profile of servicemembers, suspected intelligence officers, government officials, or private citizens. As in the case of deep fakes, this information could, in turn, be used for targeted influence operations or blackmail.