After having deployed to as many as six locations in Africa and the Middle East, personnel associated with the Defense Department's hallmark automation effort say they are learning valuable lessons in how to use algorithms for war. Known as Project Maven, the DoD initiative aims to accelerate the integration of big data and machine learning, first focusing on processing of full motion video from tactical drones and from medium altitude sensors.  Here's what officials are saying: It's critical to retrain algorithms aimed at analyzing full motion video. "It's not all that different if you think about it than a young airman coming onto my ops floor. On their first day of work it's going to take them a little while to figure out what's going on, to understand the mission, to understand the [area of responsibility] in which we're operating," Lt. Col. Garry "Pink" Floyd, deputy chief of the algorithmic warfare cross functional team, also known as Project Maven, at the Modular Open Systems Summit in Washington May 1. "You see a similar version of that with [artificial intelligence], with these algorithms." Analysts are currently swimming in troves of data that are nearly impossible for humans to sift through. To exemplify the problem, military personnel at the annual GEOINT symposium in Tampa, Florida in late April said in fiscal 2017, 127 terabytes of captured enemy data was collected. In addition, the annual video that Central Command collects could cover 325,000 feature films and the annual signals intelligence Central Command collects is equal to roughly 5.5 million songs. Floyd said in Africa Command where the first algorithm was deployed, staff retrained the machine six times in five days. "Maybe they're trained off of data from one region and we deploy it to a new region and maybe makes a few silly mistakes at first, but we've developed some tools to help do that quickly," he said, noting these algorithms have been deployed to garrison processing, exploitation and dissemination sites. Floyd explained the team built into user interfaces a button that literally says "train AI." "If you see the algorithm misidentify a palm tree as a person or something like that, the analyst, the operator can hit that train AI button, it captures the number of frames to the left of that instant, it captures the frames to right. Take that out of the theater and get it into our data labeling pipeline," he said. "We relabel the data, we rapidly get that to our algorithm developer … they retrain optimize … and then we redeploy it to the field. We were thinking this would take a long time, but again, we were able to do this in like say a day or so or even less. 