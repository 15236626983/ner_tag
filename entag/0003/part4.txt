Bayesian rule lists (BRL) is one example of interpretable models. BRL consist of series of if (condition) then (consequent) else (alternative) statements. Letham et al. [33] describes how BRL can be generated for a highly accurate and interpretable model to estimate the risk of stroke. The conditions discretize a high-dimensional multivariate feature space that influence the risk of stroke and the consequent describes the predicted risk of stroke. The BRL has similar performance as other ML-methods for predicting the risk of stroke and is just as interpretable as other existing scoring systems that are less accurate. Lexicon-based classifiers is another example of interpretable models for text classification. Lexicon-based classifiers multiplies the frequency of terms with the probability for terms occurring in each class. The class with the highest score is chosen as the prediction. Clos et al. [11] models lexicons using a gated recurrent network that jointly learns both terms and modifiers, such as adverbs and conjunctions. The lexicons where trained on whether posts in forum are for or against death penalty and sentiments towards commercial productions. The lexicons perform better than other ML-methods and are at the same time interpretable.