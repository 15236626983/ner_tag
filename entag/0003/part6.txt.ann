In contrast to classification, [@AI-planning#ai_tec*] is based on [@models of domain dynamics#ai_tec*]. Fox et al. [15] describe how explanations for planning may use [@domain models#ai_tec*] to explain why actions were performed or not, why some action cannot be performed, causal relationships that enable future actions, and the need for replanning. Since fairness is important for many [@AI-applications#ai_product*], [@Tan#Person*] et al. [59] describe how [@model distillation#ai_tec*] can be used to detect bias in [@black-box models#ai_tec*]. [@Model distillation#ai_tec*] simplifies larger more complex models without significant loss of accuracy. For transparency, they use generalized additive models based on shallow trees that model each parameter and the interaction between two parameters. They train a transparent model on system recommendations from the [@black-box model#ai_tec*] and one transparent model on the actual outcome. Hypothesis testing of differences in recommendations from the two models shows cases where the [$black-box model#ai_tec*] introduce a bias, which may then be diagnosed by comparing the two transparent models. The system was evaluated on recidivism risk, lending loan risk, and individual risk for being involved in a shooting incident. The results show that one [$black-box model#ai_tec*] underestimates recidivism risk for young criminals and Caucasians, while overestimating the risk for Native and African Americans.