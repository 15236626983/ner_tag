By O
DL B-ai_tec
we O
refer O
to O
machine B-ai_tec
learning I-ai_tec
models O
consisting O
of O
multiple O
of O
layers O
of O
nonlinear O
processing O
units. O
Typically, O
these O
models O
are O
represented O
by O
artificial B-ai_tec
neural I-ai_tec
networks I-ai_tec
. O
In O
this O
context, O
a O
neuron O
refers O
to O
a O
single O
computation O
unit O
where O
the O
output O
is O
a O
weighted O
sum O
of O
inputs O
that O
passed O
a O
(nonlinear) O
activation B-ai_tec
function I-ai_tec
(e.g., O
a O
function O
that O
passes O
the O
signal O
only O
if O
it O
is O
positive). O
DNNs B-ai_tec
refer O
to O
systems O
with O
a O
large O
number O
of O
serially O
connected O
layers O
of O
parallel-connected O
neurons B-ai_tec
. O
The O
contrast O
to O
a O
DNN B-ai_tec
is O
a O
shallow O
neural B-ai_tec
network I-ai_tec
that O
has O
only O
one O
layer O
of O
parallel-connected O
neurons B-ai_tec
. O
Until O
about O
ten O
years O
ago, O
training O
of O
DNNs B-ai_tec
was O
virtually O
impossible. O
The O
first O
successful O
training O
strategies O
for O
deep B-ai_tec
networks I-ai_tec
were O
based O
on O
training O
one O
layer O
at O
a O
time O
[21, O
6]. O
The O
parameters O
of O
the O
layer- O
by-layer-trained O
deep B-ai_tec
networks I-ai_tec
were O
finally O
fine-tuned O
(simultaneously) O
using O
stochastic B-ai_tec
gradient I-ai_tec
methods I-ai_tec
[49] O
to O
maximize O
the O
classification O
accuracy. O
Since O
then, O
many O
research O
advances O
have O
made O
it O
possible O
to O
directly O
train O
DNNs B-ai_tec
without O
having O
a O
layer-by-layer O
training. O
For O
example, O
it O
has O
been O
found O
that O
initialization O
strategies O
for O
the O
weights O
of O
the O
network O
in O
combination O
with O
activation B-ai_tec
function I-ai_tec
selection O
are O
crucial O
[16]. O
Even O
techniques O
such O
as O
randomly B-ai_tec
disabling I-ai_tec
neurons I-ai_tec
during O
the O
training O
phase O
[22], O
and O
normalizing O
the O
signals O
before O
they O
reach O
the O
activation B-ai_tec
functions I-ai_tec
[25] O
have O
shown O
to O
be O
of O
great O
importance O
in O
achieving O
good O
results O
with O
DNNs B-ai_tec
. O
Representation O
learning O
is O
one O
of O
main O
reasons O
for O
the O
high O
performance O
of O
DNNs B-ai_tec
. O
Using O
DL B-ai_tec
and O
DNNs B-ai_tec
it O
is O
no O
longer O
necessary O
to O
manually O
craft O
the O
features O
required O
to O
learn O
a O
specific O
task. O
Instead, O
discriminating O
features O
are O
automatically O
learned O
during O
the O
training O
of O
a O
DNN B-ai_tec
. O
Techniques O
and O
tools O
supporting O
DL-applications B-ai_product
are O
more O
available O
today O
than O
ever O
before. O
Advanced O
DL B-ai_tec
can O
be O
successfully O
applied O
and O
customized O
using O
only O
limited O
programming/scripting O
skills O
through O
cheap O
computational O
resources, O
free O
ML-frameworks B-ai_product
, O
pre-trained B-ai_product
models I-ai_product
, O
open-source O
data O
and O
code. O

