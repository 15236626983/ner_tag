When designing a [@DNN#ai_tec*], but only having access to a small amount of training data, it is common to use [@pre-trained models#ai_tec*] to achieve good performance. The concept is called [@transfer learning#ai_tec*] and a common procedure is to take a model that is trained on a large amount of data, replace and customize the last layers in the network to the specific problem, and then fine-tune the parameters in the final stages (and sometimes even the entire system) using the available training data. There are already a large amount of [$pre-trained models#ai_tec*] available for download from the Internet. A relevant question is then “How do we know that those who uploaded the model have no bad intentions?”. This type of vulnerability is considered in [19] where the authors insert backdoors into a model for recognizing [@US#Gpe*] traffic signs. For example, a sticker is trained on a stop sign to belong to a class other than stop signs. They then show that a system, based the [$US#Gpe*] traffic sign network, for recognizing [@Swedish#Gpe*] traffic signs reacts negatively (greatly impairing the classification accuracy of the [$Swedish#Gpe*] traffic sign system) when using the backdoor (i.e., placing a sticker on the traffic sign).