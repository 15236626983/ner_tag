Provided a [@DNN#ai_tec*], it has been found that it is easy to adjust input signal so that the classification system fails completely [58, 18, 45]. When the dimension of the input signal is large, which is typically the case for e.g. pictures, it is often enough with an imperceptible small adjustment of each element (i.e. pixel) in the input to fool the system. With the same technique used to train the [$DNN#ai_tec*], typically a [@stochastic gradient#ai_tec*] method [49], you can easily find in which direction, by looking at the sign of the gradient, each element should be changed to allow the classifier to wrongly pick a target class or simply just misclassify. With only a few lines of code, the best [@image recognition#ai_product*] systems are deceived to believe that a picture of a vehicle instead shows a dog. Figure 1 below shows the image before and after manipulation and the likelihood of the classes before and after manipulation. The above method assumes having full access to the [$DNN#ai_tec*], i.e., a [@so-called white-box attack#ai_tec*]. It has been found that even so-called [@black-box attacks#ai_tec*], where you only have insight into the systemâ€™s type of input and output, are possible [44, 56]. In [44], the authors train a [@substitute network#ai_tec*] using data obtained from sparse sampling of the [@black-box system#ai_product*] they want to attack. Given the [$substitute network#ai_tec*] you can then use the [@white-box attack#ai_tec*] method mentioned above to craft adversarial inputs. An alternative to learning a [$substitute network#ai_tec*] is presented in [56], where instead a genetic algorithm is used to create attack vectors leading to misclassifications by the system. The same authors even show that it is often enough to modify a single pixel in the image, although often perceptible, to achieve a successful attack.