By DL we refer to machine learning models consisting of multiple of layers of nonlinear processing units. Typically, these models are represented by artificial neural networks. In this context, a neuron refers to a single computation unit where the output is a weighted sum of inputs that passed a (nonlinear) activation function (e.g., a function that passes the signal only if it is positive). DNNs refer to systems with a large number of serially connected layers of parallel-connected neurons. The contrast to a DNN is a shallow neural network that has only one layer of parallel-connected neurons. Until about ten years ago, training of DNNs was virtually impossible. The first successful training strategies for deep networks were based on training one layer at a time [21, 6]. The parameters of the layer- by-layer-trained deep networks were finally fine-tuned (simultaneously) using stochastic gradient methods [49] to maximize the classification accuracy. Since then, many research advances have made it possible to directly train DNNs without having a layer-by-layer training. For example, it has been found that initialization strategies for the weights of the network in combination with activation function selection are crucial [16]. Even techniques such as randomly disabling neurons during the training phase [22], and normalizing the signals before they reach the activation functions [25] have shown to be of great importance in achieving good results with DNNs. Representation learning is one of main reasons for the high performance of DNNs. Using DL and DNNs it is no longer necessary to manually craft the features required to learn a specific task. Instead, discriminating features are automatically learned during the training of a DNN. Techniques and tools supporting DL-applications are more available today than ever before. Advanced DL can be successfully applied and customized using only limited programming/scripting skills through cheap computational resources, free ML-frameworks, pre-trained models, open-source data and code.