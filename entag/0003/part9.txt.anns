One O
way O
to O
reduce O
the O
vulnerability O
of O
the O
DNNs B-ai_tec
to O
manipulation O
of O
the O
input O
signal O
is O
to O
explicitly O
include O
manipulated/adversarial O
examples O
in O
the O
training O
process O
of O
the O
model O
[18, O
28]. O
That O
is, O
in O
addition O
to O
the O
original O
training O
data O
adversarial O
examples O
are O
generated O
and O
used O
in O
the O
training O
of O
the O
model. O
Another O
method O
is O
to O
use O
a O
concept O
called O
defense B-ai_tec
distillation I-ai_tec
[46]. O
Briefly O
described, O
the O
method O
tries O
to O
reduce O
the O
requirement O
that O
the O
output O
signal O
only O
point O
out O
the O
true O
class O
and O
force O
the O
other O
classes O
to O
have O
zero O
probability. O
This O
is O
done O
in O
[46] O
in O
two O
steps. O
The O
first O
step O
is O
a O
regular O
training O
of O
a O
DNN B-ai_tec
. O
In O
the O
second O
step, O
the O
output O
(class O
probabilities) O
of O
the O
first O
neuron B-ai_tec
network I-ai_tec
is O
used O
as O
a O
new O
class O
labels O
and O
a O
new O
system O
(with O
the O
same O
architecture) O
is O
trained O
using O
the O
new O
(soft) O
class O
labels. O
This O
has O
been O
shown O
to O
reduce O
vulnerability, O
because O
you O
do O
not O
fit O
the O
DNN B-ai_tec
too O
tight O
against O
the O
training O
data, O
and O
preserve O
some O
reasonable O
class O
interrelations. O
Other O
defense O
methods, O
are O
for O
instance O
feature O
squeezing O
techniques O
such O
as O
e.g., O
mean O
or O
median B-ai_tec
filtering I-ai_tec
[64] O
or O
nonlinear O
pixel O
representations O
such O
as O
one-hot B-ai_tec
or O
thermometer B-ai_tec
encodings I-ai_tec
[8]. O
Unfortunately, O
neither O
of O
the O
methods O
described O
completely O
solves O
the O
vulnerability O
problem, O
especially O
not O
if O
the O
attacker O
has O
full O
insight O
into O
the O
model O
and O
the O
defense O
method. O

