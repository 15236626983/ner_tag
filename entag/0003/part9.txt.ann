One way to reduce the vulnerability of the [@DNNs#ai_tec*] to manipulation of the input signal is to explicitly include manipulated/adversarial examples in the training process of the model [18, 28]. That is, in addition to the original training data adversarial examples are generated and used in the training of the model. Another method is to use a concept called [@defense distillation#ai_tec*] [46]. Briefly described, the method tries to reduce the requirement that the output signal only point out the true class and force the other classes to have zero probability. This is done in [46] in two steps. The first step is a regular training of a [@DNN#ai_tec*]. In the second step, the output (class probabilities) of the first [@neuron network#ai_tec*] is used as a new class labels and a new system (with the same architecture) is trained using the new (soft) class labels. This has been shown to reduce vulnerability, because you do not fit the [$DNN#ai_tec*] too tight against the training data, and preserve some reasonable class interrelations. Other defense methods, are for instance feature squeezing techniques such as e.g., mean or [@median filtering#ai_tec*] [64] or nonlinear pixel representations such as [@one-hot#ai_tec*] or [@thermometer encodings#ai_tec*] [8]. Unfortunately, neither of the methods described completely solves the vulnerability problem, especially not if the attacker has full insight into the model and the defense method.