Although DNNs offer high performance in many applications, their sub-symbolic computations with perhaps millions of parameters makes it difficult to understand exactly how input features contribute to system recommendations. Since DNNs high performance is critical for many applications, there is a considerable interest in how to make them more interpretable (see [39] for a review). Many algorithms for interpreting DNNs transform the DNN-processing into the original input space in order to visualize discriminating features. Typically, two general approaches are used for feature visualization, activation maximation and DNN explanation. Activation maximation computes which inputs features that will maximally activate possible system recommendations. For image classification, this represents the ideal images that show discriminating and recognizable features for each class. However, the images often look unnatural since the classes may use many aspects of the same object and the semantic information in images is often spread out [43]. Some examples of methods for activation maximation are gradient ascent [13], better regularization to increase generalizability [54], and synthesizing preferred images [41, 40]. DNN explanation explains system recommendations by highlighting discriminating input features. In image classification, such visualizations may highlight areas that provide evidence for or against a certain class [68] or only show regions that contain discriminating features [3]. One approach for calculating discriminating features is sensitivity analysis using local gradients or other measure of variation [39]. However, one problem with sensitivity analysis is that it may indicate discriminating features that are not present in the input. For example, in image classification the sensitivity analysis may indicate obscured parts of an object rather than the visible parts [51]. Layer-wise relevance propagation avoids this problem by considering both feature presence and model reaction [4].