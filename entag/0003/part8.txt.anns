When O
designing O
a O
DNN B-ai_tec
, O
but O
only O
having O
access O
to O
a O
small O
amount O
of O
training O
data, O
it O
is O
common O
to O
use O
pre-trained B-ai_tec
models I-ai_tec
to O
achieve O
good O
performance. O
The O
concept O
is O
called O
transfer B-ai_tec
learning I-ai_tec
and O
a O
common O
procedure O
is O
to O
take O
a O
model O
that O
is O
trained O
on O
a O
large O
amount O
of O
data, O
replace O
and O
customize O
the O
last O
layers O
in O
the O
network O
to O
the O
specific O
problem, O
and O
then O
fine-tune O
the O
parameters O
in O
the O
final O
stages O
(and O
sometimes O
even O
the O
entire O
system) O
using O
the O
available O
training O
data. O
There O
are O
already O
a O
large O
amount O
of O
pre-trained B-ai_tec
models I-ai_tec
available O
for O
download O
from O
the O
Internet. O
A O
relevant O
question O
is O
then O
“How O
do O
we O
know O
that O
those O
who O
uploaded O
the O
model O
have O
no O
bad O
intentions?”. O
This O
type O
of O
vulnerability O
is O
considered O
in O
[19] O
where O
the O
authors O
insert O
backdoors O
into O
a O
model O
for O
recognizing O
US B-Gpe
traffic O
signs. O
For O
example, O
a O
sticker O
is O
trained O
on O
a O
stop O
sign O
to O
belong O
to O
a O
class O
other O
than O
stop O
signs. O
They O
then O
show O
that O
a O
system, O
based O
the O
US B-Gpe
traffic O
sign O
network, O
for O
recognizing O
Swedish B-Gpe
traffic O
signs O
reacts O
negatively O
(greatly O
impairing O
the O
classification O
accuracy O
of O
the O
Swedish B-Gpe
traffic O
sign O
system) O
when O
using O
the O
backdoor O
(i.e., O
placing O
a O
sticker O
on O
the O
traffic O
sign). O

