Provided O
a O
DNN B-ai_tec
, O
it O
has O
been O
found O
that O
it O
is O
easy O
to O
adjust O
input O
signal O
so O
that O
the O
classification O
system O
fails O
completely O
[58, O
18, O
45]. O
When O
the O
dimension O
of O
the O
input O
signal O
is O
large, O
which O
is O
typically O
the O
case O
for O
e.g. O
pictures, O
it O
is O
often O
enough O
with O
an O
imperceptible O
small O
adjustment O
of O
each O
element O
(i.e. O
pixel) O
in O
the O
input O
to O
fool O
the O
system. O
With O
the O
same O
technique O
used O
to O
train O
the O
DNN B-ai_tec
, O
typically O
a O
stochastic B-ai_tec
gradient I-ai_tec
method O
[49], O
you O
can O
easily O
find O
in O
which O
direction, O
by O
looking O
at O
the O
sign O
of O
the O
gradient, O
each O
element O
should O
be O
changed O
to O
allow O
the O
classifier O
to O
wrongly O
pick O
a O
target O
class O
or O
simply O
just O
misclassify. O
With O
only O
a O
few O
lines O
of O
code, O
the O
best O
image B-ai_product
recognition I-ai_product
systems O
are O
deceived O
to O
believe O
that O
a O
picture O
of O
a O
vehicle O
instead O
shows O
a O
dog. O
Figure O
1 O
below O
shows O
the O
image O
before O
and O
after O
manipulation O
and O
the O
likelihood O
of O
the O
classes O
before O
and O
after O
manipulation. O
The O
above O
method O
assumes O
having O
full O
access O
to O
the O
DNN B-ai_tec
, O
i.e., O
a O
so-called B-ai_tec
white-box I-ai_tec
attack I-ai_tec
. O
It O
has O
been O
found O
that O
even O
so-called O
black-box B-ai_tec
attacks I-ai_tec
, O
where O
you O
only O
have O
insight O
into O
the O
systemâ€™s O
type O
of O
input O
and O
output, O
are O
possible O
[44, O
56]. O
In O
[44], O
the O
authors O
train O
a O
substitute B-ai_tec
network I-ai_tec
using O
data O
obtained O
from O
sparse O
sampling O
of O
the O
black-box B-ai_product
system I-ai_product
they O
want O
to O
attack. O
Given O
the O
substitute B-ai_tec
network I-ai_tec
you O
can O
then O
use O
the O
white-box B-ai_tec
attack I-ai_tec
method O
mentioned O
above O
to O
craft O
adversarial O
inputs. O
An O
alternative O
to O
learning O
a O
substitute B-ai_tec
network I-ai_tec
is O
presented O
in O
[56], O
where O
instead O
a O
genetic O
algorithm O
is O
used O
to O
create O
attack O
vectors O
leading O
to O
misclassifications O
by O
the O
system. O
The O
same O
authors O
even O
show O
that O
it O
is O
often O
enough O
to O
modify O
a O
single O
pixel O
in O
the O
image, O
although O
often O
perceptible, O
to O
achieve O
a O
successful O
attack. O

