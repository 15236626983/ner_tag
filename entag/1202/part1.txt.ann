After a while the buzzwords start to ring hollow. What's "[@artificial intelligence#ai_tec*]" in practical terms? An Orwellian nightmare that will control our every battlefield maneuver? Or a helpful tool to aid the war fighter? Let's bring it down to Earth, make it tangible. [@AI#ai_tec*] can, for instance, scan a live video feed faster and more accurately than any human and then warn commanders of imminent danger.At least that's the premise behind an ongoing project at the [@Air Force Research Laboratory#MISC*] at [@Wright-Patterson Air Force Base#MISC*]. "Video is captured with such velocity and volume [that] no individual or team of individuals can hope to analyze that data in a meaningful way," said [@Scott Clouse#Person*], senior research engineer at the Decision Science Branch. [@AI#ai_tec*] offers a faster, smarter alternative. While video intelligence can be an invaluable resource, it's also a massive manpower suck. "In some instances, we are up to teams of 30 people looking at one video feed, just to make sure we don't miss anything," [@Clouse#Person*] said. "There is an immediate need to cut down the number of people on a single feed, maybe even to the point where we could have a single person looking at multiple feeds. It could dramatically reduce the workload on the force."In [@late 2017#Date*] the [@AFRL research team#MISC*] hit a milestone in its work, winning the Large-Scale Movie Description Challenge at the [@2017#Date*] [@International Conference on Computer Vision#MISC*] in [@Venice#Gpe*], [@Italy#Gpe*]. In that competition, [@AI-driven systems#ai_product*] were tasked with creating simple written descriptions of short clips taken from commercial film footage. The techniques used here could in principal serve as the basis for an [@AI-driven#ai_tec*] situational awareness tool. While the team kept its movie captions deliberately terse — "Someone looks up, someone reads a letter" — video interpretation on the battlefield could perhaps offer an even deeper dive into video intelligence."On the battlefield you are not competing with the other audio, you are not competing with music in the background. You could deliver a more lengthy verbal description," said [@Vincent Velten#Person*], [@AFRL#Org*]'s [@Multi-Domain Sensing Autonomy Division Decision Science Branch#Org*] technical advisor.For [@AI#ai_tec*] to interpret video data, the system must be told what to look for. Right now, programmers can set a simple system to identify specific shapes or colors or types of activity. Moving forward, they want the [@AI-driven system#ai_product*] to cull more detailed information. In technical terms, the mechanism that enables this is called a [@recurrent neural net#ai_tec*], or [@RNN#ai_tec*].